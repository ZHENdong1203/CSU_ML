{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "决策树是基于树结构来进行决策的，类似人在面临决策问题时一种很自然的处理机制。下图是一个决策树的例子，关于女孩决定是否去相亲：\n",
    "![女孩决定是否去相亲决策树](images/image01.png)\n",
    "\n",
    "一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定序列。使用决策树进行决策的过程就是从根结点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子结点，将叶子结点存放的类别作为决策结果。\n",
    "\n",
    "## 生成决策树\n",
    "\n",
    "决策树算法的伪代码：\n",
    "\n",
    "输入： 训练集 $D = \\{(x_1, y_1),(x_2, y_2),\\cdots,(x_m, y_m)\\}$；\n",
    "     属性集 $A = \\{a_1, a_2,\\cdots,a_m\\}$。\n",
    "\n",
    "过程：函数generate_tree(D, A)\n",
    "\n",
    "```pascal\n",
    "生成结点 node；\n",
    "if D 中样本全属于同一类别C then\n",
    "    将 node 标记为C类叶结点；return\n",
    "end if\n",
    "if A = 空集 then\n",
    "    将 node 标记为叶结点，其类别标记为D中样本数最多的类；return\n",
    "end if\n",
    "从A中选择最优划分属性a；\n",
    "for a 的每一个值 v do\n",
    "    为 node 生成一个分支；令D_v表示D中在a上取值v的样本子集\n",
    "    if D_v 为空 then\n",
    "        将分支结点标记为叶结点，其类别标记为D中样本最多的类；return\n",
    "    else\n",
    "        以 generate_tree(D_v, A \\ {a})为分支结点\n",
    "    end if\n",
    "end for\n",
    "```\n",
    "\n",
    "输出：以 node 为根结点的一棵决策树。\n",
    "\n",
    "\n",
    "在从A中选择最优划分属性a中，根据策略的不同，决策树可以分为ID3、C4.5和CART。\n",
    "\n",
    "\n",
    "首先，我们来定义决策树使用的数据类型：结点。结点分为内部结点和叶子结点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    结点的父类\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_classes_num = None  # 训练集类分布\n",
    "        self.val_classes_num = None  # 验证集类分布\n",
    "        self.data_info = None  # 训练集信息\n",
    "\n",
    "class InternalNode(Node):\n",
    "    \"\"\"\n",
    "    内部结点\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature = None  # 属性名称\n",
    "        self.axis = None  # 属性在训练集中的位置\n",
    "        self.branch = []  # 结点的分支\n",
    "        self.feature_values = [] # 属性的取值，位置与branch对应\n",
    "        \n",
    "    def __str__(self):\n",
    "        print_str = \"{\" + \"\\\"\" + self.feature + \"\\\"\" + \": {\"\n",
    "        for i in range(len(self.branch)):\n",
    "            print_str += str(\"\\\"\" + self.feature_values[i] + \"\\\"\" + \": \" + str(self.branch[i])) + \", \"\n",
    "        print_str = print_str[:-2]\n",
    "        print_str += \"}}\"\n",
    "        return print_str\n",
    "    \n",
    "\n",
    "class LeafNode(Node):\n",
    "    \"\"\"\n",
    "    叶子结点\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.class_label = None  # class label\n",
    "        #self.val_set = []  # 保存验证集样本\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\\"\" + self.class_label + \"\\\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析函数`generate_tree`，我们需要一个函数`get_max_num_class`来获取训练集中数量最多的类，需要函数`split_data_set`来划分训练集，需要函数`get_best_feature`来选择最优划分属性，需要函数`count_values`统计属性或类别标签在训练集中的值的分布。我们接下来实现这4个函数。\n",
    "\n",
    "**任务1：**实现`get_max_num_class`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_num_class(classes_num):\n",
    "    \"\"\"\n",
    "    获取数量最多的类\n",
    "    参数：\n",
    "        classes_num: 类别标签的分布，比如[1,0,2,3]\n",
    "    返回：\n",
    "        数量最多的类的位置\n",
    "    \"\"\"\n",
    "    \n",
    "    cls = None\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    cls=classes_num.index(max(classes_num))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "test = [1,0,2,3]\n",
    "print(get_max_num_class(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务2：**实现`split_data_set`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_set(data_set, axis, value, is_delete_axis=True):\n",
    "    \"\"\"\n",
    "    在训练集的axis列，选择值为value的训练集样本，并返回\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        axis： 属性所在列\n",
    "        value： 属性的所取的值\n",
    "        is_delete_axis: 是否删除第axis列\n",
    "    返回：\n",
    "        划分的子训练集\n",
    "    \"\"\"\n",
    "    sub_data_set = []\n",
    "    ### START CODE HERE ###\n",
    "    for sample in data_set:\n",
    "        if sample[axis] == value:\n",
    "            if is_delete_axis:\n",
    "                # 删除第axis列并把sample保存到sub_data_set中\n",
    "                axisSample =  sample[:axis]                #去掉axis特征\n",
    "                axisSample.extend(sample[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "                sub_data_set.append(axisSample)\n",
    "            else:\n",
    "                # 把sample保存到sub_data_set中\n",
    "                sub_data_set.append(sample)\n",
    "                \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return sub_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [1]]\n"
     ]
    }
   ],
   "source": [
    "test = [[1,1],[1,2],[2,3],[2,1],[3,2],[3,3]]\n",
    "print(split_data_set(test, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数`count_values`统计数据集中某一列的值的分布。\n",
    "\n",
    "**任务3：**实现`count_values`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(data_set, axis, values):\n",
    "    \"\"\"\n",
    "    统计values在数据集中第axis列出现的个数\n",
    "    参数：\n",
    "        data_set: 数据集\n",
    "        axis: 位置\n",
    "        values: 统计的值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    value_list = [sample[axis] for sample in data_set]\n",
    "    values_num = []\n",
    "    for i in range(len(values)):\n",
    "        # 统计特征值values【i】出现的次数\n",
    "        count=0\n",
    "        for label in value_list:\n",
    "            if label == values[i]:\n",
    "                count = count+1\n",
    "        values_num.append(count)\n",
    "    ### END CODE HERE ###\n",
    "    return values_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "test =  [[1,1],[1,2],[2,3],[2,1],[3,2],[3,3]]\n",
    "print(count_values(test, -1, [1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数`get_best_feature`选择最优划分属性，有许多划分策略，现在实现一个简单的策略：选择属性集中第一个属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature(data_set, features, data_info):\n",
    "    \"\"\"\n",
    "    根据某种策略选择最优划分属性\n",
    "    这里选择第一个属性\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        features： 属性集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        选择的属性的位置axis\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务4：**实现`generate_tree`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tree(data_set, features, get_best_feature, data_info):\n",
    "    \"\"\"\n",
    "    生成决策树\n",
    "    参数：\n",
    "        data_set: 训练集，最后一列是类别标签，其他是属性eatures\n",
    "        features: 属性集\n",
    "        get_best_feature: 选择最优划分属性的函数\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        决策树的结点\n",
    "    \"\"\"\n",
    "    assert len(data_set) != 0, \"the size of data set cannot be 0\"\n",
    "    \n",
    "    features = features[:]\n",
    "    \n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    val_classes_num = [0 for i in classes_num]\n",
    "    \n",
    "    class_list = [sample[-1] for sample in data_set]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # 如果训练集的样本都属于同一类，返回类别标记为这个类的叶子结点\n",
    "    if len(set(class_list)) == 1:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = data_info[\"classes\"][get_max_num_class(classes_num)]\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "\n",
    "    # 如果属性集为空，返回类别标记为训练集中数量最多的类的叶子结点\n",
    "    if len(features) == 0:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = data_info[\"classes\"][get_max_num_class(classes_num)]\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "\n",
    "    node = InternalNode()\n",
    "    node.data_info = data_info\n",
    "    node.train_classes_num = classes_num\n",
    "    node.val_classes_num = val_classes_num\n",
    "    # 选择最优划分属性a\n",
    "    axis = get_best_feature(data_set, features, data_info)\n",
    "    best_feature = features[axis]\n",
    "    node.feature = best_feature\n",
    "    node.axis = data_info[\"features\"].index(best_feature)\n",
    "    del features[axis]\n",
    "\n",
    "    feature_values = data_info[\"features_values\"][best_feature]\n",
    "    node.feature_values = feature_values\n",
    "    # 根据属性a取值划分训练集\n",
    "    for feature_value in feature_values:\n",
    "        sub_data_set = split_data_set(data_set, axis, feature_value)\n",
    "        if len(sub_data_set) == 0:\n",
    "            sub_node = LeafNode()\n",
    "            sub_node.data_info = data_info\n",
    "            sub_node.class_label = data_info[\"classes\"][get_max_num_class(classes_num)]\n",
    "            sub_node.train_classes_num = count_values(sub_data_set, -1, data_info[\"classes\"])\n",
    "            sub_node.val_classes_num = val_classes_num\n",
    "            node.branch.append(sub_node)\n",
    "        else:\n",
    "            # 为子集生成决策树\n",
    "            node.branch.append(generate_tree(sub_data_set, features, get_best_feature, data_info))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经完成了函数`generate_tree`，让我们导入数据并生成一个决策树。我们使用的数据来自[http://archive.ics.uci.edu/ml/datasets/Balance+Scale](http://archive.ics.uci.edu/ml/datasets/Balance+Scale)。该数据用来模拟心理学实验结果，它的一条记录为：\n",
    "\n",
    "1. Class Name: 3 (L, B, R) \n",
    "2. Left-Weight: 5 (1, 2, 3, 4, 5) \n",
    "3. Left-Distance: 5 (1, 2, 3, 4, 5) \n",
    "4. Right-Weight: 5 (1, 2, 3, 4, 5) \n",
    "5. Right-Distance: 5 (1, 2, 3, 4, 5)\n",
    "\n",
    "分类的正确方法是选择$\\text{Left-Weight} \\times \\text{Left-Distance}$和$\\text{Right-Weight} \\times \\text{Right-Distance}$中的较大者。如果相等，那么类别为B。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1', '1', 'B']\n",
      "['1', '1', '1', '2', 'R']\n",
      "['1', '1', '1', '3', 'R']\n",
      "['1', '1', '1', '4', 'R']\n",
      "['1', '1', '1', '5', 'R']\n",
      "['1', '1', '2', '1', 'R']\n",
      "['1', '1', '2', '2', 'R']\n",
      "['1', '1', '2', '3', 'R']\n",
      "['1', '1', '2', '4', 'R']\n",
      "['1', '1', '2', '5', 'R']\n",
      "{\n",
      "  \"LW\": {\n",
      "    \"1\": {\n",
      "      \"LD\": {\n",
      "        \"1\": {\n",
      "          \"RW\": {\n",
      "            \"1\": {\n",
      "              \"RD\": {\n",
      "                \"1\": \"B\",\n",
      "                \"2\": \"R\",\n",
      "                \"3\": \"R\",\n",
      "                \"4\": \"R\",\n",
      "                \"5\": \"R\"\n",
      "              }\n",
      "            },\n",
      "            \"2\": \"R\",\n",
      "            \"3\": \"R\",\n",
      "            \"4\": \"R\",\n",
      "            \"5\": \"R\"\n",
      "          }\n",
      "        },\n",
      "        \"2\": \"R\",\n",
      "        \"3\": \"R\",\n",
      "        \"4\": \"R\",\n",
      "        \"5\": \"R\"\n",
      "      }\n",
      "    },\n",
      "    \"2\": \"R\",\n",
      "    \"3\": \"R\",\n",
      "    \"4\": \"R\",\n",
      "    \"5\": \"R\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(\"balance-scale.data\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "row_list = content.splitlines()\n",
    "data_set = [row.split(\",\") for row in row_list if row.strip()]\n",
    "# 调整类别标签和属性的位置\n",
    "for sample in data_set:\n",
    "    class_label = sample.pop(0)\n",
    "    sample.append(class_label)\n",
    "\n",
    "features = [\"LW\", \"LD\", \"RW\", \"RD\"]\n",
    "feature_values = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "features_values = {\"LW\": feature_values, \"LD\": feature_values, \"RW\": feature_values, \"RD\": feature_values}\n",
    "classes = [\"L\", \"B\",  \"R\"]\n",
    "data_info = {\"features\": features, \"features_values\": features_values, \"classes\": classes}\n",
    "\n",
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "    \n",
    "decision_tree = generate_tree(data_set[:10], features, get_best_feature, data_info)\n",
    "print(json.dumps(eval(str(decision_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，先前实现的`get_best_feature`使用的选择最优划分属性策略不好，生成的决策树太复杂。因此，需要选择好的策略。\n",
    "\n",
    "## ID3\n",
    "\n",
    "熵(entropy)：信息量大小的度量，即表示随机变量不确定性的度量。\n",
    "\n",
    "事件$a_i$的信息量$I(a_i)$可如下度量：$I(a_i) = -p(a_i) \\log p(a_i)$，这里$p(a_i)$为事件$a_i$发生的概率。\n",
    "\n",
    "假设有n个互不相容的事件$a_i, \\cdots, a_n$，它们中有且仅有一个发生，则其平均的信息量（熵）可如下度量：\n",
    "$$\n",
    "I(a_i, \\cdots, a_n) = \\sum_{i} I(a_i) = - \\sum_{i} p(a_i) \\log p(a_i)\n",
    "$$\n",
    "\n",
    "假设当前训练集D中第k类样本比例为$p_k$，对应的信息熵为：\n",
    "$$\n",
    "\\text{Ent}(D) = - \\sum_k p_k \\log p_k\n",
    "$$\n",
    "\n",
    "$\\text{Ent}(D)$越小，表示数据越有序，纯度越高，分类效果越好。\n",
    "\n",
    "假设某离散属性a有V个可能值，若采用该属性对样本集来划分，则会产生V个分支，每个分支节点包含的数据记为$D^v$。用属性a对训练集D进行划分，获得的信息增益为：\n",
    "$$\n",
    "\\text{Gain}(D, a) = \\text{Ent}(D) - \\sum_v \\frac{|D^v|}{|D|} \\text{Ent}(D^v)\n",
    "$$\n",
    "\n",
    "ID3选择具有最大信息增益的属性来划分：$a* = \\arg \\underset{a}{\\max} \\text{Gain}(D, a)$。\n",
    "\n",
    "接下来实现ID3版本的选择最优划分属性函数`get_best_feature_id3`。首先实现计算训练集信息熵函数`compute_entropy`。\n",
    "\n",
    "**任务5：**实现`compute_entropy`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(values_num):\n",
    "    \"\"\"\n",
    "    根据变量的分布values_num计算信息熵\n",
    "    参数：\n",
    "        values_num: 变量的分布\n",
    "    返回：\n",
    "        信息熵\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    m = float(sum(values_num))\n",
    "    entropy = 0\n",
    "    for value_num in values_num:\n",
    "        if value_num == 0:  # 不能计算 log(0)\n",
    "            continue\n",
    "        prob =  value_num / m\n",
    "        entropy -= prob * np.log(prob)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0114042647073518\n"
     ]
    }
   ],
   "source": [
    "test = [1,2,3]\n",
    "print(compute_entropy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务6：**实现`get_best_feature_id3`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_id3(data_set, features, data_info):\n",
    "    \"\"\"\n",
    "    根据信息增益选择划分属性\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        features： 属性集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        选择的属性的位置axis\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # 计算训练集的信息熵\n",
    "    m = len(data_set)\n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    entropy = compute_entropy(classes_num)\n",
    "\n",
    "    gain = 0\n",
    "    best_axis = 0\n",
    "    # 计算按照属性a的取值划分训练集后的信息熵\n",
    "    for i in range(len(features)):\n",
    "        split_entropy = 0\n",
    "        feature_values = data_info[\"features_values\"][features[i]]\n",
    "        for feature_value in feature_values:\n",
    "            sub_data_set = split_data_set(data_set, i, feature_values, is_delete_axis=True)\n",
    "            sub_classes_num = count_values(data_set,  -1, data_info[\"classes\"])\n",
    "            inforS = len(sub_data_set) * 1.0\n",
    "            split_entropy +=  compute_entropy(sub_classes_num) * inforS / m\n",
    "        if entropy - split_entropy > gain:\n",
    "            gain = entropy - split_entropy\n",
    "            best_axis = i\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return best_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1', '1', 'B']\n",
      "['1', '1', '1', '2', 'R']\n",
      "['1', '1', '1', '3', 'R']\n",
      "['1', '1', '1', '4', 'R']\n",
      "['1', '1', '1', '5', 'R']\n",
      "['1', '1', '2', '1', 'R']\n",
      "['1', '1', '2', '2', 'R']\n",
      "['1', '1', '2', '3', 'R']\n",
      "['1', '1', '2', '4', 'R']\n",
      "['1', '1', '2', '5', 'R']\n",
      "{\n",
      "  \"LW\": {\n",
      "    \"1\": {\n",
      "      \"LD\": {\n",
      "        \"1\": {\n",
      "          \"RW\": {\n",
      "            \"1\": {\n",
      "              \"RD\": {\n",
      "                \"1\": \"B\",\n",
      "                \"2\": \"R\",\n",
      "                \"3\": \"R\",\n",
      "                \"4\": \"R\",\n",
      "                \"5\": \"R\"\n",
      "              }\n",
      "            },\n",
      "            \"2\": \"R\",\n",
      "            \"3\": \"R\",\n",
      "            \"4\": \"R\",\n",
      "            \"5\": \"R\"\n",
      "          }\n",
      "        },\n",
      "        \"2\": \"R\",\n",
      "        \"3\": \"R\",\n",
      "        \"4\": \"R\",\n",
      "        \"5\": \"R\"\n",
      "      }\n",
      "    },\n",
      "    \"2\": \"R\",\n",
      "    \"3\": \"R\",\n",
      "    \"4\": \"R\",\n",
      "    \"5\": \"R\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "    \n",
    "id3_tree = generate_tree(data_set[:10], features, get_best_feature_id3, data_info)\n",
    "print(json.dumps(eval(str(id3_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，ID3生成的决策树比之前生成的决策树要简单得多。现在可以使用生成的决策树对新样本进行分类。\n",
    "\n",
    "**任务7：**实现决策树的推理函数`inference`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(tree, sample, is_add_to_node=False):\n",
    "    \"\"\"\n",
    "    对新样本进行分类\n",
    "    参数：\n",
    "        sample: 新样本\n",
    "        is_add_to_node: 是否把新样本类别信息放到结点中\n",
    "    返回：\n",
    "        新样本的分类结果\n",
    "    \"\"\"\n",
    "    node = tree\n",
    "    classes = tree.data_info[\"classes\"]\n",
    "    if is_add_to_node:\n",
    "        node.val_classes_num[classes.index(sample[-1])] += 1\n",
    "        \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    while type(node) == InternalNode:\n",
    "        for i in range(len(node.feature_values)):\n",
    "            if node.feature_values[i] == sample[node.axis]:\n",
    "                node = node.branch[i]\n",
    "                break\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node.class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "class_label = inference(id3_tree, data_set[0], data_info)\n",
    "print(class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5\n",
    "\n",
    "ID3算法的基本思想是以信息增益选择属性，实际应用中会对可能取值数目较多的属性有所偏好。例如，如果对每个训练样本进行编号，并将编号作为属性，其信息增益最大，但显然该属性不能作为分类依据。因此，C4.5算法使用信息增益比来选择划分属性。\n",
    "\n",
    "信息增益比公式如下：\n",
    "$$\n",
    "\\text{Gain_ratio}(D, a) = \\frac{\\text{Gain}(D,a)}{\\text{IV}(a)}\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "\\text{IV}(a) = - \\sum_{v=1}^V \\frac{|D^v|}{|D|} \\log \\frac{|D^v|}{|D|}\n",
    "$$\n",
    "是属性a的固定值(intrinsic value)，a可能的取值越多，$\\text{IV}(a)$通常也越大。\n",
    "\n",
    "接下来实现C4.5版本的选择最优划分属性函数`get_best_feature_c45`。在这里不设置信息增益比的阈值$\\epsilon$，直接选择具有最大信息增益比的属性。\n",
    "\n",
    "**任务8：**实现`get_best_feature_c45`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_c45(data_set, features, data_info):\n",
    "    \"\"\"\n",
    "    根据信息增益率选择划分属性\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        features： 属性集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        选择的属性的位置axis\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # 计算训练集的信息熵\n",
    "    m = len(data_set)\n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    entropy = compute_entropy(classes_num)\n",
    "\n",
    "    gain_ratio = 0\n",
    "    best_axis = 0\n",
    "    # 计算按照属性a的取值划分训练集后的信息熵\n",
    "    for i in range(len(features)):\n",
    "        split_entropy = 0\n",
    "        gain = 0\n",
    "        feature_values_num = count_values(data_set, i, data_info[\"features_values\"][features[i]])\n",
    "        # iv + 1e-7 防止分母为0\n",
    "        iv = compute_entropy(feature_values_num) + 1e-7\n",
    "        feature_values = data_info[\"features_values\"][features[i]]\n",
    "        for feature_value in feature_values:\n",
    "            sub_data_set = split_data_set(data_set, i, feature_value)\n",
    "            sub_classes_num = count_values(sub_data_set, -1, data_info[\"classes\"])\n",
    "            inforS = len(sub_data_set)\n",
    "            split_entropy += compute_entropy(sub_classes_num) * inforS / m\n",
    "        gain = entropy - split_entropy\n",
    "        if gain / iv > gain_ratio:\n",
    "            gain_ratio = gain / iv\n",
    "            best_axis = i\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return best_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1', '1', 'B']\n",
      "['1', '1', '1', '2', 'R']\n",
      "['1', '1', '1', '3', 'R']\n",
      "['1', '1', '1', '4', 'R']\n",
      "['1', '1', '1', '5', 'R']\n",
      "['1', '1', '2', '1', 'R']\n",
      "['1', '1', '2', '2', 'R']\n",
      "['1', '1', '2', '3', 'R']\n",
      "['1', '1', '2', '4', 'R']\n",
      "['1', '1', '2', '5', 'R']\n",
      "{\n",
      "  \"RD\": {\n",
      "    \"1\": {\n",
      "      \"RW\": {\n",
      "        \"1\": \"B\",\n",
      "        \"2\": \"R\",\n",
      "        \"3\": \"B\",\n",
      "        \"4\": \"B\",\n",
      "        \"5\": \"B\"\n",
      "      }\n",
      "    },\n",
      "    \"2\": \"R\",\n",
      "    \"3\": \"R\",\n",
      "    \"4\": \"R\",\n",
      "    \"5\": \"R\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "    \n",
    "c45_tree = generate_tree(data_set[:10], features, get_best_feature_c45, data_info)\n",
    "print(json.dumps(eval(str(c45_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为数据简单，C4.5生成的决策树和ID3生成的决策树一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "通常生成的决策树复杂度非常高，过度拟合训练数据，导致过拟合，决策树的剪枝减少决策树的规模，是处理过拟合问题的主要手段。\n",
    "\n",
    "通过极小化决策树整体的 Cost Complexity 函数实现剪枝\n",
    "$$\n",
    "\\text{CC}(T) = \\text{Err}(T) + \\lambda R(T)\n",
    "$$\n",
    "其中$\\text{Err}(T)$为树T的错误，$R(T)$为正则化项，描述树的复杂度（如树节点的个数），$\\lambda \\ge 0$为正则化参数。\n",
    "\n",
    "设树T的叶结点个数为$|T|$，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$。叶结点t上的信息熵：\n",
    "$$\n",
    "H_t(T) = - \\sum_{k=1}^K \\frac{N_{tk}}{N_t} \\log \\frac{N_{tk}}{N_t} \n",
    "$$\n",
    "$\\text{Err}(T)$为\n",
    "$$\n",
    "\\text{Err}(T) = \\sum_{i=1}^{|T|} N_t H_t(T) = - \\sum_{i=1}^{|T|} \\sum_{k=1}^K N_{tk} \\log \\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "则\n",
    "$$\n",
    "\\text{CC}(T) = - \\sum_{i=1}^{|T|} \\sum_{k=1}^K N_{tk} N_{tk} \\log \\frac{N_{tk}}{N_t}  + \\lambda |T|\n",
    "$$\n",
    "\n",
    "决策树的剪枝算法：\n",
    "\n",
    "- 输入：算法产生的整个决策树T，正则化参数$\\lambda$\n",
    "- 输出：修剪后的决策树T\n",
    "    - Step 1: 计算每个结点的代价\n",
    "    - Step 2: 递归地从树的叶结点向上回溯\n",
    "- 设一组叶结点回溯到其父结点之前与之后的代价分别为：$\\text{CC}(T_B)$和$\\text{CC}(T_A)$，若$\\text{CC}(T_A) \\le \\text{CC}(T_B)$，则剪枝，返回Step 2，直到不能继续为止。\n",
    "- 注意是根据验证集上的代价来决定是否剪枝。\n",
    "\n",
    "**任务9：**下面我们来实现计算 Cost Complexity 的函数`compute_tree_cc`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tree_cc(tree, lambd=1):\n",
    "    \"\"\"\n",
    "    计算决策树的 Cost Complexity 值\n",
    "    参数：\n",
    "        tree: 决策树\n",
    "        lambd: 正则化参数\n",
    "    返回：\n",
    "        决策树的 Cost Complexity 值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    err = 0\n",
    "    leaf_count = 0\n",
    "    node_list = [tree]\n",
    "    while len(node_list) != 0:\n",
    "        node = node_list.pop(0)\n",
    "        if type(node) == InternalNode:\n",
    "            for i in node.branch:\n",
    "                node_list.append(i)\n",
    "        else:\n",
    "            Nt = sum(node.val_classes_num)\n",
    "            ent = compute_entropy(node.val_classes_num)\n",
    "            err += ent * Nt\n",
    "            leaf_count += 1\n",
    "    \n",
    "    return err + lambd * leaf_count\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(compute_tree_cc(c45_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务10：**实现决策树的剪枝函数`pruning`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(tree, p, node, lambd = 1):\n",
    "    \"\"\"\n",
    "    剪枝决策树\n",
    "    参数：\n",
    "        tree: 决策树\n",
    "        p: 父结点\n",
    "        node: 正在遍历的结点\n",
    "        lambd: 正则化参数\n",
    "    返回：\n",
    "        处理后的结点\n",
    "    \"\"\"\n",
    "    if type(node) == LeafNode:\n",
    "        return node\n",
    "    # 后序遍历\n",
    "    for i in range(len(node.branch)):\n",
    "        node.branch[i] = pruning(tree, node, node.branch[i], lambd)\n",
    "    \n",
    "    can_pruning = True\n",
    "    for i in node.branch:\n",
    "        if type(i) == InternalNode:\n",
    "            can_pruning = False\n",
    "            break\n",
    "    \n",
    "    if can_pruning:\n",
    "        CC_before = compute_tree_cc(tree, lambd)\n",
    "        CC_after = None\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # 内部结点转叶子结点\n",
    "        leaf_node = LeafNode()\n",
    "        leaf_node.train_classes_num = node.train_classes_num\n",
    "        leaf_node.val_classes_num = node.val_classes_num\n",
    "        leaf_node.data_info = node.data_info\n",
    "        classes = node.data_info[\"classes\"]\n",
    "        classes_num = node.train_classes_num\n",
    "        leaf_node.class_label = classes[get_max_num_class(classes_num)]\n",
    "        \n",
    "        if tree == node:\n",
    "            CC_after = compute_tree_cc(leaf_node, lambd)\n",
    "        else:\n",
    "            index = p.branch.index(node)\n",
    "            p.branch[index] = leaf_node\n",
    "            CC_after = compute_tree_cc(tree, lambd)\n",
    "            \n",
    "        if CC_after <= CC_before:\n",
    "            return leaf_node\n",
    "        else:\n",
    "            return node\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    else:\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前：\n",
      "the accuracy before pruning is 0.606\n",
      "剪枝后：\n",
      "the accuracy before pruning is 0.532\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集和验证集\n",
    "m = len(data_set)\n",
    "m_train = int(m*0.7)\n",
    "m_val = m - m_train\n",
    "data_train = data_set[:m_train]\n",
    "data_val = data_set[m_train:]\n",
    "classes_label_val = [sample[-1] for sample in data_val]\n",
    "\n",
    "c45_tree_before = generate_tree(data_train, features, get_best_feature_c45, data_info)\n",
    "print(\"剪枝前：\")\n",
    "#print(json.dumps(eval(str(c45_tree_before)), sort_keys=True, indent=2))\n",
    "\n",
    "predict = []\n",
    "for sample in data_val:\n",
    "    predict.append(inference(c45_tree_before, sample, is_add_to_node=True))\n",
    "\n",
    "is_same = [1 if classes_label_val[i] == predict[i] else 0 for i in range(m_val)]\n",
    "accuracy = sum(is_same) / m_val\n",
    "print(\"the accuracy before pruning is %.3f\" % accuracy)\n",
    "\n",
    "# 剪枝\n",
    "c45_tree_after = pruning(c45_tree_before, None, c45_tree_before, lambd=1)\n",
    "print(\"剪枝后：\")\n",
    "#print(json.dumps(eval(str(c45_tree_after)), sort_keys=True, indent=2))\n",
    "\n",
    "predict = []\n",
    "for sample in data_val:\n",
    "    predict.append(inference(c45_tree_after, sample, is_add_to_node=False))\n",
    "\n",
    "is_same = [1 if classes_label_val[i] == predict[i] else 0 for i in range(m_val)]\n",
    "accuracy = sum(is_same) / m_val\n",
    "print(\"the accuracy before pruning is %.3f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "\n",
    "CART(Classification and Regression Tree)算法是目前决策树算法中最为成熟的一类算法，它既可以用于分类（CART决策树），又可以用于预测（CART回归树）。\n",
    "\n",
    "CART决策树与ID3、C4.5的不同之处在于CART决策树是一棵二叉树，使用基尼系数（Gini Index）来选择划分属性。\n",
    "\n",
    "基尼值的计算公式为：\n",
    "$$\n",
    "\\text{Gini}(D) = \\sum_{k=1}^{K} \\sum_{k' \\not = k} p_k p_{k'} = 1 - \\sum_{k=1}^K p_k^2\n",
    "$$\n",
    "\n",
    "直观上，基尼值反应了从数据集中任选2个样本，其类别不一致的概率，其值越小，纯度越高。\n",
    "\n",
    "基尼系数的计算公式为：\n",
    "$$\n",
    "\\text{Gini_index}(D, a) = \\sum_{v=1}^V \\frac{|D^v|}{|D|} \\text{Gini}(D^v)\n",
    "$$\n",
    "\n",
    "CART决策树使用的是离散值的属性，如果样本的属性是连续型的，需要使用离散化方法把连续属性离散化。\n",
    "\n",
    "CART决策树生成算法：\n",
    "\n",
    "- 输入：训练数据集D，迭代终止条件\n",
    "- 输出：CART决策树\n",
    "- 从根结点开始，递归对每个结点进行以下操作构造二叉树\n",
    "    - 设结点数据集为D，对每个特征A，对其每个值a，根据样本点对$A=a$的测试为是或否，将D分为$D^-$,$D^+$，计算$A=a$的基尼系数\n",
    "    - 在所有的特征A以及所有可能的切分点a中，选择基尼指数最小的特征和切分点，将数据集分配到两个子结点中\n",
    "    - 对两个子结点递归调用上面两个步骤，最终生成CART决策树\n",
    "    \n",
    "下面开始实现CART决策树，我们需要`compute_gini`计算基尼值，`compute_gini_index`计算基尼系数。\n",
    "\n",
    "**任务11：**实现`compute_gini`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gini(class_num):\n",
    "    \"\"\"\n",
    "    计算基尼值\n",
    "    参数：\n",
    "        class_num: 类别的分布信息\n",
    "    返回：\n",
    "        基尼值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    gini = 0\n",
    "    m = float(sum(class_num))\n",
    "    for i in class_num:\n",
    "        if i != 0:\n",
    "            prob = i / m\n",
    "            gini += prob ** 2\n",
    "    return 1 - gini\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "test = [1, 2, 3]\n",
    "print(compute_gini(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务12：**实现`compute_gini_index`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gini_index(data_set, axis, value, data_info):\n",
    "    \"\"\"\n",
    "    计算(D, a)基尼系数\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        axis: 属性位置\n",
    "        value: 选取的属性值\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        基尼系数\n",
    "    \"\"\"\n",
    "    m = len(data_set)\n",
    "    \n",
    "    classes = data_info[\"classes\"]\n",
    "    \n",
    "    feature = data_info[\"features\"][axis]\n",
    "    feature_values = data_info[\"features_values\"][feature]\n",
    "    \n",
    "    values_num = count_values(data_set, axis, feature_values)\n",
    "    \n",
    "    value_index = feature_values.index(value)\n",
    "    value_count = values_num[value_index]\n",
    "    others_count = m - value_count\n",
    "    ### START CODE HERE ###\n",
    "    # compute gini of value\n",
    "    sub_data_set = split_data_set(data_set, axis, value, is_delete_axis=False)\n",
    "    value_gini = compute_gini(count_values(sub_data_set, -1, data_info[\"classes\"]))\n",
    "    \n",
    "    # compute gini of others\n",
    "    sub_data_set = []\n",
    "    for feature_value in feature_values:\n",
    "        if feature_value == value:\n",
    "            continue\n",
    "        sub_data_set.extend(split_data_set(data_set, axis, feature_value, is_delete_axis=False))\n",
    "    others_gini = compute_gini(count_values(sub_data_set, -1, data_info[\"classes\"]))\n",
    "    \n",
    "    return  (value_gini * value_count + others_gini * others_count) / m\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test = [\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 2],\n",
    "    [2, 1, 1],\n",
    "    [2, 2, 2]\n",
    "]\n",
    "\n",
    "test_data_info = {\n",
    "    \"features\": [1, 2],\n",
    "    \"features_values\": {1: [1, 2], 2: [1, 2]},\n",
    "    \"classes\": [1, 2]\n",
    "}\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(1, 3):\n",
    "        print(compute_gini_index(test, i, j, test_data_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数`get_best_feature_cart_decision`用于获取基尼系数最小的属性和切分点。\n",
    "\n",
    "**任务13：**实现`get_best_feature_cart_decision`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_cart_decision(data_set, data_info):\n",
    "    \"\"\"\n",
    "    根据基尼系数选择最优划分属性和切分点\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        (axis, vlaue): 属性位置，切分点值\n",
    "    \"\"\"\n",
    "    m = len(data_set)\n",
    "    \n",
    "    lowest_gini_index = 1\n",
    "    axis = 0\n",
    "    value = None\n",
    "    \n",
    "    classes = data_info[\"classes\"]\n",
    "    \n",
    "    features = data_info[\"features\"]\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(len(features)):\n",
    "        feature = features[i]\n",
    "        gini_index = 0\n",
    "        \n",
    "        feature_values = data_info[\"features_values\"][feature]\n",
    "        values_num = count_values(data_set, i, feature_values)\n",
    "        # 该属性在data_set中只有一个值，不能把数据分成两部分\n",
    "        if m in values_num:\n",
    "            continue\n",
    "        \n",
    "        for j in range(len(feature_values)):\n",
    "            if values_num[j] == 0:\n",
    "                continue\n",
    "            gini_index = compute_gini_index(data_set, i, feature_values[j], data_info)\n",
    "            \n",
    "            if gini_index < lowest_gini_index:\n",
    "                lowest_gini_index = gini_index\n",
    "                axis = i\n",
    "                value = feature_values[j]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return (axis, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "test = [\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 2],\n",
    "    [2, 1, 1],\n",
    "    [2, 2, 2]\n",
    "]\n",
    "\n",
    "test_data_info = {\n",
    "    \"features\": [1, 2],\n",
    "    \"features_values\": {1: [1, 2], 2: [1, 2]},\n",
    "    \"classes\": [1, 2]\n",
    "}\n",
    "\n",
    "print(get_best_feature_cart_decision(test, test_data_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务14：**实现`generate_cart_classifier`函数，生成CART决策树。迭代终止条件有许多种，比如限制树的最大深度、限制叶子结点的个数等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cart_classifier(data_set, data_info):\n",
    "    \"\"\"\n",
    "    生成CART决策树\n",
    "    使用默认终止条件\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        data_info: 训练集信息\n",
    "    返回：\n",
    "        决策树的结点\n",
    "    \"\"\"\n",
    "    m = len(data_set)\n",
    "    assert m != 0, \"the size of data set cannot be 0\"\n",
    "    \n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    val_classes_num = [0] * len(classes_num)\n",
    "    \n",
    "    # 如果训练集的样本都属于同一类，返回类别标记为这个类的叶子结点\n",
    "    if m in classes_num:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = data_set[0][-1]\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "    \n",
    "    # 如果训练集的每种属性都取相同的值，返回类别标记为训练集中数量最多的类的叶子结点\n",
    "    is_stop = True\n",
    "    features = data_info[\"features\"]\n",
    "    for feature in features:\n",
    "        feature_values = data_info[\"features_values\"][feature]\n",
    "        values_num = count_values(data_set, features.index(feature), feature_values)\n",
    "        if m not in values_num:\n",
    "            is_stop = False\n",
    "            break\n",
    "    if is_stop:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = data_info[\"classes\"][get_max_num_class(classes_num)]\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    node = InternalNode()\n",
    "    node.data_info = data_info\n",
    "    node.train_classes_num = classes_num\n",
    "    node.val_classes_num = val_classes_num\n",
    "    # 选择使基尼系数最小的属性和切分点\n",
    "    axis, value = get_best_feature_cart_decision(data_set, data_info)\n",
    "    best_feature = features[axis]\n",
    "    node.feature = best_feature\n",
    "    node.axis = axis\n",
    "    node.feature_values = [value, \"other\"]\n",
    "    \n",
    "    feature_values = data_info[\"features_values\"][best_feature]\n",
    "    # 左子树（value）\n",
    "    sub_data_set_value = split_data_set(data_set, axis, value, is_delete_axis=False)\n",
    "    node.branch.append(generate_cart_classifier(sub_data_set_value, data_info))\n",
    "    \n",
    "    # 右子树（others）\n",
    "    sub_data_set_others = []\n",
    "    for feature_value in feature_values:\n",
    "        if feature_value == value:\n",
    "            continue\n",
    "        sub_data_set_others.extend(split_data_set(data_set, axis, feature_value, is_delete_axis=False))\n",
    "    node.branch.append(generate_cart_classifier(sub_data_set_others, data_info))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1', '1', 'B']\n",
      "['1', '1', '1', '2', 'R']\n",
      "['1', '1', '1', '3', 'R']\n",
      "['1', '1', '1', '4', 'R']\n",
      "['1', '1', '1', '5', 'R']\n",
      "['1', '1', '2', '1', 'R']\n",
      "['1', '1', '2', '2', 'R']\n",
      "['1', '1', '2', '3', 'R']\n",
      "['1', '1', '2', '4', 'R']\n",
      "['1', '1', '2', '5', 'R']\n",
      "{\n",
      "  \"RD\": {\n",
      "    \"1\": {\n",
      "      \"RW\": {\n",
      "        \"1\": \"B\",\n",
      "        \"other\": \"R\"\n",
      "      }\n",
      "    },\n",
      "    \"other\": \"R\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "\n",
    "cart_tree = generate_cart_classifier(data_set[:10], data_info)\n",
    "print(json.dumps(eval(str(cart_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务15：**实现`cart_classifier_inference`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_classifier_inference(tree, sample):\n",
    "    \"\"\"\n",
    "    对sample进行预测\n",
    "    参数：\n",
    "        tree: 分类树 \n",
    "        sampel: 样本\n",
    "    返回：\n",
    "        y_predict: 预测结果\n",
    "    \"\"\"\n",
    "    node = tree\n",
    "    ### START CODE HERE ###\n",
    "    while type(node) == InternalNode:\n",
    "        if sample[node.axis] == node.feature_values[0]:\n",
    "            node = node.branch[0]\n",
    "        else:\n",
    "            node = node.branch[1]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node.class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "print(cart_classifier_inference(cart_tree, data_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART回归树\n",
    "\n",
    "一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元$R_1, R_2, \\cdots, R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M} c_m I(x \\in R_m)\n",
    "$$\n",
    "\n",
    "当输入空间的划分确定时，可以用平方误差$\\sum_{x_i \\in R_m} (y_i - f(x_i))^2$来表示回归树对与训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值，即\n",
    "$$\n",
    "\\hat{c}_m = \\text{avg} (y_i | x_i \\in R_m)\n",
    "$$\n",
    "\n",
    "对输入空间的划分方法如下。选择第j个变量$x^{(j)}$和它取的值s，作为切分变量和切分点，并定义两个区域：\n",
    "$$\n",
    "R_1(j,s) = \\{ x | x^{(j)} \\le s \\} \\text{ 和 } R_1(j,s) = \\{ x | x^{(j)} \\gt s \\}\n",
    "$$\n",
    "然后寻找最优切分变量j和最优切分点s。具体地，求解\n",
    "$$\n",
    "\\underset{j,s}{\\min} \\left [ \\underset{c_1}{\\min} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\underset{c_2}{\\min} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2 \\right ]\n",
    "$$\n",
    "对固定输入变量j可以找到最优切分点s。\n",
    "$$\n",
    "\\hat{c}_1 = \\text{avg}(y_i | x_i \\in R_1(j, s)) \\text{ 和 } \\hat{c}_2 = \\text{avg}(y_i | x_i \\in R_2(j, s))\n",
    "$$\n",
    "遍历所有输入变量，找到最优的切分变量j，构成一对$(j,s)$。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树。\n",
    "\n",
    "最小二乘回归树算法：  \n",
    "输入：训练数据集D；  \n",
    "输出：回归树$f(x)$。  \n",
    "在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：  \n",
    "（1）选择最优切分变量j和切分点s，求解：\n",
    "$$\n",
    "\\underset{j,s}{\\min} \\left [ \\underset{c_1}{\\min} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\underset{c_2}{\\min} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2 \\right ]\n",
    "$$\n",
    "遍历变量j，对固定的切分变量j扫描切分点s，选择使上式达到最小值的对$(j,s)$。  \n",
    "（2）用选定的对$(j,s)$划分区域并决定相应的输出值：\n",
    "$$\n",
    "R_1(j,s) = \\{ x | x^{(j)} \\le s \\}, R_1(j,s) = \\{ x | x^{(j)} \\gt s \\} \\\\\n",
    "\\hat{c}_m = \\frac{1}{N_m} \\sum_{x_i \\in R_m(j,s)} y_i, x \\in R_m, m=1,2\n",
    "$$\n",
    "（3）继续对两个子区域调用步骤（1）（2），直至满足停止条件。  \n",
    "（4）将输入空间划分为M个区域$R_1, R_2, \\cdots, R_M$，生成决策树：\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M} \\hat{c}_m I(x \\in R_m)\n",
    "$$\n",
    "\n",
    "下面开始实现CART回归树。首先定义回归树用到的数据类型，因为之前的数据类型Node不适合CART回归树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNode(object):\n",
    "    \"\"\"\n",
    "    CART回归树使用的结点类型\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = \"leaf\"  # 结点类型m，\"leaf\": 叶子结点，\"internal\": 内部结点\n",
    "        self.y = None\n",
    "        self.splitting_variable = None  # 切分变量，特征位置\n",
    "        self.splitting_point = None  # 切分点\n",
    "        self.predict = None  # 输出值\n",
    "        self.left = None  # 左结点\n",
    "        self.right = None  # 右结点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务16：**实现分割训练集的函数`split_X`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(X, y, j, s):\n",
    "    \"\"\"\n",
    "    根据切分变量j和切分点s划分训练集X，y\n",
    "    参数：\n",
    "        X: 训练集\n",
    "        y: 训练集标签\n",
    "        j: 切分变量\n",
    "        s: 切分点\n",
    "    返回:\n",
    "        ((X_1, y_1),(X_2, y_2)): 划分后的训练集\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    indicate = X[:, j] <= s\n",
    "    X_1 = X[indicate == True]\n",
    "    y_1 = y[indicate == True]\n",
    "    \n",
    "    X_2 = X[indicate == False]\n",
    "    y_2 = y[indicate == False]\n",
    "    \n",
    "    return ((X_1, y_1), (X_2, y_2))\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "[0 1 2]\n",
      "((array([[0, 1, 2, 3],\n",
      "       [4, 5, 6, 7]]), array([0, 1])), (array([[ 8,  9, 10, 11]]), array([2])))\n"
     ]
    }
   ],
   "source": [
    "test_X = np.arange(12).reshape(3, 4)\n",
    "test_y = np.array([i for i in range(3)])\n",
    "print(test_X)\n",
    "print(test_y)\n",
    "print(split_X(test_X, test_y, 1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务17：**实现选择最优切分变量j和切分点s的函数`get_splitting_variable_and_point`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitting_variable_and_point(X, y):\n",
    "    \"\"\"\n",
    "    选择最优切分变量j和切分点s\n",
    "    参数：\n",
    "        X: 训练集\n",
    "        y: 训练标签\n",
    "    返回：\n",
    "        (j, s): 最优切分变量j和切分点s\n",
    "    \"\"\"\n",
    "    lowest_loss = sys.maxsize\n",
    "    splitting_variable = 0\n",
    "    splitting_point = 1\n",
    "    m, n = X.shape\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    for j in range(n):\n",
    "        values = list(set(X[:, j]))\n",
    "        values.sort()\n",
    "        for s in range(len(values) - 1):\n",
    "            ((X_1, y_1), (X_2, y_2)) = split_X(X, y, j, values[s])\n",
    "            c1 = np.mean(y_1)\n",
    "            c2 = np.mean(y_2)\n",
    "            loss1 = ((y_1 - c1)**2).sum()\n",
    "            loss2 = ((y_2 - c2)**2).sum()\n",
    "            loss = loss1 + loss2\n",
    "            if loss < lowest_loss:\n",
    "                lowest_loss = loss\n",
    "                splitting_variable = j\n",
    "                splitting_point = values[s]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return (splitting_variable, splitting_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "test_X = np.array([[1],[2],[3],[4],[5]])\n",
    "test_y = np.array([1,1,1,2,2])\n",
    "\n",
    "print(get_splitting_variable_and_point(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务18：**实现CART回归树的生成函数`generate_cart_regression`，终止条件是决策树达到树的最大深度max_depth。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cart_regressor(X, y, depth, max_depth):\n",
    "    \"\"\"\n",
    "    生成CART回归树。\n",
    "    参数：\n",
    "        X: 训练集\n",
    "        y: 训练集标签\n",
    "        depth: 返回结点的深度\n",
    "        max_depth: 树的最大深度\n",
    "    \"\"\"\n",
    "    node = RNode()\n",
    "    node.y = y\n",
    "    # 该结点的最优输出是y的均值\n",
    "    node.predict = np.mean(y)\n",
    "    m, n = X.shape\n",
    "    # 达到最大深度，生成叶子结点\n",
    "    if depth == max_depth:\n",
    "        node.type = \"leaf\"\n",
    "        return node\n",
    "    \n",
    "    y_vales = set(y[:,0])\n",
    "    # 所有y值都相同\n",
    "    if len(y_vales) == 1:\n",
    "        node.type = \"leaf\"\n",
    "        return node\n",
    "    \n",
    "    # 如果训练集的每种属性都取相同的值\n",
    "    is_stop = True\n",
    "    for i in range(n):\n",
    "        feature_values = set(X[:, i])\n",
    "        if len(set(feature_values)) != 1:\n",
    "            is_stop = False\n",
    "            break\n",
    "    if is_stop:\n",
    "        node.type = \"leaf\"\n",
    "        return node\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    node.type = \"internal\"\n",
    "    j, s = get_splitting_variable_and_point(X,y)\n",
    "    node.splitting_variable = j\n",
    "    node.splitting_point = s\n",
    "    ((X_1, y_1), (X_2, y_2)) = split_X(X, y, j, s)\n",
    "    node.left = generate_cart_regressor(X_1, y_1, depth + 1, max_depth)\n",
    "    node.right = generate_cart_regressor(X_2, y_2, depth + 1, max_depth)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务19：**实现`cart_regressor_inference`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_regressor_inference(cart_regressor, x):\n",
    "    \"\"\"\n",
    "    对X进行预测\n",
    "    参数：\n",
    "        cart_regressor: 回归树\n",
    "        X: 预测集\n",
    "    返回：\n",
    "        y_predict: 预测结果\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    node = cart_regressor\n",
    "    while node.type == \"internal\":\n",
    "        if x[node.splitting_variable] <= node.splitting_point:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node.predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用CART回归树模拟sin函数\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.linspace(-5, 5, 200)\n",
    "y = np.sin(X)\n",
    "y = y + np.random.rand(1, len(y)) * 1.5 # 加入噪声\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "cart_regressor = generate_cart_regressor(X, y, 1, 5)\n",
    "\n",
    "X_train = np.arange(-5, 5, 0.05).reshape(-1, 1)\n",
    "y_predict = np.zeros((X_train.shape[0], 1))\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    y_predict[i, 0] = cart_regressor_inference(cart_regressor, X[i])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y,c=\"k\", label=\"data\")\n",
    "plt.plot(X_train, y_predict, c=\"r\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面不是实验内容，有兴趣可以了解下CART的剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART剪枝\n",
    "\n",
    "CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列${T_0, T_1, \\cdots, T_n}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。\n",
    "\n",
    "定义子树的损失函数为：\n",
    "$$\n",
    "C_{\\alpha}(T) = C(T) + \\alpha |T|\n",
    "$$\n",
    "其中$C(T)$表示对训练数据的预测误差（如基尼值），$|T|$为子树的叶子结点的个数。\n",
    "对T的任意内部结点t，以t为单结点树的损失函数是：\n",
    "$$\n",
    "C_{\\alpha}(t) = C(t) + \\alpha\n",
    "$$\n",
    "以t为根结点的子树$T_t$的损失函数是：\n",
    "$$\n",
    "C_{\\alpha}(T_t) = C(T_t) + \\alpha |T_t|\n",
    "$$\n",
    "\n",
    "CART剪枝算法如下：\n",
    "\n",
    "输入：CART算法生成的决策树$T_0$:  \n",
    "输出：最优决策树$T_{\\alpha}$：  \n",
    "（1）设$k=0$，$T = T_0$  \n",
    "（2）设$\\alpha = +\\infty$  \n",
    "（3）自下而上地对各内部结点t计算$C(T_t)$，$|T_t|$以及\n",
    "$$\n",
    "g(t) = \\frac{C(t) - C(T_t)}{|T_t| - 1} \\\\\n",
    "\\alpha = \\min(\\alpha, g(t))\n",
    "$$\n",
    "这里$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。  \n",
    "（4）对$g(t) = \\alpha$的内部结点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T。  \n",
    "（5）设$k = k+1$，$\\alpha_k = \\alpha$，$T_k = T$。  \n",
    "（6）如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤（3）；否则令$T_k = T_n$。  \n",
    "（7）采用交叉验证法在子树序列$T_0, T_1, \\cdots, T_n$中选取最优子树$T_{\\alpha}$。\n",
    "\n",
    "算法的详细解释请看李航的《统计学习方法》的5.5.2部分内容。\n",
    "\n",
    "回归树的剪枝算法与决策树的剪枝算法类似。我们同时实现CART分类树和回归树的剪枝算法。首先实现计算$C(t)$和$C(T_t)$的函数`compute_node_loss`和`compute_tree_loss`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_loss(node, tree_type):\n",
    "    \"\"\"\n",
    "    计算C(t)的值\n",
    "    参数：\n",
    "        node：内部结点\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回:\n",
    "        C(t)的值\n",
    "    \"\"\"\n",
    "    if tree_type == \"classifier\":\n",
    "        m = sum(node.train_classes_num)\n",
    "        return m * compute_gini(node.train_classes_num)\n",
    "    if tree_type == \"regressor\":\n",
    "        return np.sum(np.power(node.y - node.predict, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tree_loss_and_leaf(root, tree_type):\n",
    "    \"\"\"\n",
    "    计算C(T_t)的值和树的叶子结点个数\n",
    "    参数：\n",
    "        root: 树的根结点\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回：\n",
    "        C(T_t)的值和树的叶子结点个数\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    leaf_node_num = 0\n",
    "    node_list = [root]\n",
    "    while len(node_list) != 0:\n",
    "        node = node_list.pop(0)\n",
    "        if tree_type == \"classifier\":\n",
    "            if type(node) == InternalNode:\n",
    "                node_list.extend(node.branch)\n",
    "            else:\n",
    "                leaf_node_num += 1\n",
    "                loss += compute_gini(node.train_classes_num)\n",
    "        if tree_type == \"regressor\":\n",
    "            if node.type == \"internal\":\n",
    "                node_list.append(node.left)\n",
    "                node_list.append(node.right)\n",
    "            else:\n",
    "                leaf_node_num += 1\n",
    "                loss += np.sum(np.power(node.y - node.predict, 2))\n",
    "                \n",
    "    return (loss, leaf_node_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现`get_optimal_tree`，用于自下而上地对各内部结点t计算$C(T_t)$，$|T_t|$以及\n",
    "$$\n",
    "g(t) = \\frac{C(t) - C(T_t)}{|T_t| - 1} \\\\\n",
    "\\alpha = \\min(\\alpha, g(t))\n",
    "$$\n",
    "这里$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_tree(tree, p, alpha, tree_type):\n",
    "    \"\"\"\n",
    "    对每个alpha区间，找到最优子树\n",
    "    参数：\n",
    "        tree: CART树\n",
    "        p: 父结点\n",
    "        alpha: 目前alpha的值\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回：\n",
    "        (alpha, p, t): 更新后的alpha值， t的父结点p， 选择的内部结点t\n",
    "    \"\"\"\n",
    "    best_alpha = alpha\n",
    "    best_p = None\n",
    "    best_t = None\n",
    "    if tree_type == \"classifier\":\n",
    "        if type(tree.branch[0]) == InternalNode:\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.branch[0], tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        if type(tree.branch[1]) == InternalNode:\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.branch[1], tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        node_loss = compute_node_loss(tree, tree_type)\n",
    "        tree_loss, num = compute_tree_loss_and_leaf(tree, tree_type)\n",
    "        gt = (node_loss - tree_loss) / (num - 1)\n",
    "        if gt < best_alpha:\n",
    "            best_alpha, best_p, best_t = gt, p, tree\n",
    "    \n",
    "    if tree_type == \"regressor\":\n",
    "        if tree.left.type == \"internal\":\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.left, tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        if tree.right.type == \"internal\":\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.right, tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        node_loss = compute_node_loss(tree, tree_type)\n",
    "        tree_loss, num = compute_tree_loss_and_leaf(tree, tree_type)\n",
    "        gt = (node_loss - tree_loss) / (num - 1)\n",
    "        if gt < best_alpha:\n",
    "            best_alpha, best_p, best_t = gt, p, tree\n",
    "    \n",
    "    return (best_alpha, best_p, best_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_cart(tree0, val_X, val_y, tree_type):\n",
    "    \"\"\"\n",
    "    对CART树进行剪枝\n",
    "    参数：\n",
    "        tree0: CART树T_0\n",
    "        val_X: 交叉验证集X\n",
    "        val_y: 交叉验证集y\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回：\n",
    "        最优子树T\n",
    "    \"\"\"\n",
    "    if tree_type == \"classifier\":\n",
    "        if type(tree0) == LeafNode:\n",
    "            return tree0\n",
    "        if type(tree0.branch[0]) == LeafNode and type(tree0.branch[1]) == LeafNode:\n",
    "            return tree0\n",
    "    if tree_type == \"regressor\":\n",
    "        if tree0.type == \"leaf\":\n",
    "            return tree0\n",
    "        if tree0.left.type == \"leaf\" and tree0.right.type == \"leaf\":\n",
    "            return tree0\n",
    "    \n",
    "    T_list = [tree0]\n",
    "    alpha_list = [0]\n",
    "    while True:\n",
    "        T = copy.deepcopy(T_list[-1])\n",
    "        root = T\n",
    "        alpha, p, best_t = get_optimal_tree(T, None, sys.maxsize, tree_type)\n",
    "        # 裁剪t\n",
    "        t = None\n",
    "        if tree_type == \"classifier\":\n",
    "            t = LeafNode()\n",
    "            t.train_classes_num = best_t.train_classes_num\n",
    "            t.val_classes_num = best_t.val_classes_num\n",
    "            t.data_info = best_t.data_info\n",
    "            t.classs_label = get_max_num_class(t.train_classes_num)\n",
    "        if tree_type == \"regressor\":\n",
    "            t = RNode()\n",
    "            t.type = \"leaf\"\n",
    "            t.y = best_t.y\n",
    "            t.predict = best_t.predict\n",
    "            \n",
    "        alpha_list.append(alpha)\n",
    "        # 根结点也可以是叶子结点\n",
    "        if best_t == root:\n",
    "            T_list.append(t)\n",
    "            break\n",
    "        else:\n",
    "            if tree_type == \"classifier\":\n",
    "                if p.branch[0] == best_t:\n",
    "                    p.branch[0] = t\n",
    "                if p.branch[1] == best_t:\n",
    "                    p.branch[1] = t\n",
    "            if tree_type == \"regressor\":\n",
    "                if p.left == best_t:\n",
    "                    p.left = t\n",
    "                if p.right == best_t:\n",
    "                    p.right = t\n",
    "            T_list.append(T)\n",
    "\n",
    "    # 采用交叉验证法在子树序列T_list中选取最优子树T\n",
    "    best_T = None\n",
    "    if tree_type == \"classifier\":        \n",
    "        best_acc = 0\n",
    "        for T in T_list:\n",
    "            y_predict = []\n",
    "            for i in range(len(val_X)):\n",
    "                predict = cart_classifier_inference(T, val_X[i])\n",
    "                y_predict.append(1 if predict == val_y[i] else 0)\n",
    "            acc = np.mean(y_predict)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_T = T\n",
    "    if tree_type == \"regressor\":\n",
    "        best_loss = sys.maxsize\n",
    "        for T in T_list:\n",
    "            y_predict = []\n",
    "            for i in range(len(val_X)):\n",
    "                y_predict.append(cart_regressor_inference(T, val_X[i]))\n",
    "            y_predict = np.array(y_predict).reshape(-1, 1)\n",
    "            loss = np.mean(np.power(val_y - y_predict, 2))\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_T = T\n",
    "       \n",
    "    return best_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用CART回归树模拟sin函数\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.linspace(-5, 5, 200)\n",
    "y = np.sin(X)\n",
    "y = y + np.random.rand(1, len(y)) * 1.5 # 加入噪声\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "np.random.seed(0)\n",
    "indicate = np.random.rand(200) <= 0.7\n",
    "\n",
    "X_train = X[indicate].reshape(-1, 1)\n",
    "y_train = y[indicate].reshape(-1, 1)\n",
    "\n",
    "X_val = X[~indicate].reshape(-1, 1)\n",
    "y_val = y[~indicate].reshape(-1, 1)\n",
    "\n",
    "regressor_before = generate_cart_regressor(X_train, y_train, 1, 5)\n",
    "\n",
    "X_show = np.arange(-5, 5, 0.05).reshape(-1, 1)\n",
    "y_predict_before = np.zeros((X_show.shape[0], 1))\n",
    "\n",
    "for i in range(X_show.shape[0]):\n",
    "    y_predict_before[i, 0] = cart_regressor_inference(regressor_before, X_show[i])\n",
    "\n",
    "regressor_after = pruning_cart(regressor_before, X_val, y_val, \"regressor\")\n",
    "\n",
    "y_predict_after = np.zeros((X_show.shape[0], 1))\n",
    "\n",
    "for i in range(X_show.shape[0]):\n",
    "    y_predict_after[i, 0] = cart_regressor_inference(regressor_before, X_show[i])\n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "plt.scatter(X, y,c=\"k\", label=\"data\")\n",
    "plt.plot(X_show, y_predict_before, c=\"r\", label=\"before\", linewidth=2)\n",
    "plt.plot(X_show, y_predict_before, c=\"b\", label=\"after\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"RW\": {\n",
      "    \"1\": {\n",
      "      \"LD\": {\n",
      "        \"1\": {\n",
      "          \"RD\": {\n",
      "            \"1\": {\n",
      "              \"LW\": {\n",
      "                \"1\": \"B\",\n",
      "                \"other\": \"L\"\n",
      "              }\n",
      "            },\n",
      "            \"other\": {\n",
      "              \"LW\": {\n",
      "                \"4\": {\n",
      "                  \"RD\": {\n",
      "                    \"4\": \"B\",\n",
      "                    \"other\": {\n",
      "                      \"RD\": {\n",
      "                        \"5\": \"R\",\n",
      "                        \"other\": \"L\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                },\n",
      "                \"other\": {\n",
      "                  \"RD\": {\n",
      "                    \"2\": {\n",
      "                      \"LW\": {\n",
      "                        \"1\": \"R\",\n",
      "                        \"other\": {\n",
      "                          \"LW\": {\n",
      "                            \"2\": \"B\",\n",
      "                            \"other\": \"L\"\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": {\n",
      "                      \"LW\": {\n",
      "                        \"3\": {\n",
      "                          \"RD\": {\n",
      "                            \"3\": \"B\",\n",
      "                            \"other\": \"R\"\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": \"R\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"other\": {\n",
      "          \"LW\": {\n",
      "            \"1\": {\n",
      "              \"RD\": {\n",
      "                \"5\": {\n",
      "                  \"LD\": {\n",
      "                    \"5\": \"B\",\n",
      "                    \"other\": \"R\"\n",
      "                  }\n",
      "                },\n",
      "                \"other\": {\n",
      "                  \"LD\": {\n",
      "                    \"2\": {\n",
      "                      \"RD\": {\n",
      "                        \"1\": \"L\",\n",
      "                        \"other\": {\n",
      "                          \"RD\": {\n",
      "                            \"2\": \"B\",\n",
      "                            \"other\": \"R\"\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": {\n",
      "                      \"RD\": {\n",
      "                        \"4\": {\n",
      "                          \"LD\": {\n",
      "                            \"3\": \"R\",\n",
      "                            \"other\": {\n",
      "                              \"LD\": {\n",
      "                                \"4\": \"B\",\n",
      "                                \"other\": \"L\"\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": {\n",
      "                          \"LD\": {\n",
      "                            \"3\": {\n",
      "                              \"RD\": {\n",
      "                                \"3\": \"B\",\n",
      "                                \"other\": \"L\"\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": \"L\"\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"other\": {\n",
      "              \"LD\": {\n",
      "                \"2\": {\n",
      "                  \"LW\": {\n",
      "                    \"2\": {\n",
      "                      \"RD\": {\n",
      "                        \"4\": \"B\",\n",
      "                        \"other\": {\n",
      "                          \"RD\": {\n",
      "                            \"5\": \"R\",\n",
      "                            \"other\": \"L\"\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": \"L\"\n",
      "                  }\n",
      "                },\n",
      "                \"other\": \"L\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"other\": {\n",
      "      \"RD\": {\n",
      "        \"1\": {\n",
      "          \"LD\": {\n",
      "            \"1\": {\n",
      "              \"LW\": {\n",
      "                \"4\": {\n",
      "                  \"RW\": {\n",
      "                    \"4\": \"B\",\n",
      "                    \"other\": {\n",
      "                      \"RW\": {\n",
      "                        \"5\": \"R\",\n",
      "                        \"other\": \"L\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                },\n",
      "                \"other\": {\n",
      "                  \"RW\": {\n",
      "                    \"2\": {\n",
      "                      \"LW\": {\n",
      "                        \"1\": \"R\",\n",
      "                        \"other\": {\n",
      "                          \"LW\": {\n",
      "                            \"2\": \"B\",\n",
      "                            \"other\": \"L\"\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": {\n",
      "                      \"LW\": {\n",
      "                        \"3\": {\n",
      "                          \"RW\": {\n",
      "                            \"3\": \"B\",\n",
      "                            \"other\": \"R\"\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": \"R\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"other\": {\n",
      "              \"LW\": {\n",
      "                \"1\": {\n",
      "                  \"LD\": {\n",
      "                    \"2\": {\n",
      "                      \"RW\": {\n",
      "                        \"2\": \"B\",\n",
      "                        \"other\": \"R\"\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": {\n",
      "                      \"RW\": {\n",
      "                        \"5\": {\n",
      "                          \"LD\": {\n",
      "                            \"5\": \"B\",\n",
      "                            \"other\": \"R\"\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": {\n",
      "                          \"LD\": {\n",
      "                            \"3\": {\n",
      "                              \"RW\": {\n",
      "                                \"2\": \"L\",\n",
      "                                \"other\": {\n",
      "                                  \"RW\": {\n",
      "                                    \"3\": \"B\",\n",
      "                                    \"other\": \"R\"\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": {\n",
      "                              \"RW\": {\n",
      "                                \"4\": {\n",
      "                                  \"LD\": {\n",
      "                                    \"4\": \"B\",\n",
      "                                    \"other\": \"L\"\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": \"L\"\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                },\n",
      "                \"other\": {\n",
      "                  \"LD\": {\n",
      "                    \"2\": {\n",
      "                      \"LW\": {\n",
      "                        \"2\": {\n",
      "                          \"RW\": {\n",
      "                            \"4\": \"B\",\n",
      "                            \"other\": {\n",
      "                              \"RW\": {\n",
      "                                \"5\": \"R\",\n",
      "                                \"other\": \"L\"\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": \"L\"\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": \"L\"\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"other\": {\n",
      "          \"RW\": {\n",
      "            \"2\": {\n",
      "              \"LW\": {\n",
      "                \"1\": {\n",
      "                  \"RD\": {\n",
      "                    \"2\": {\n",
      "                      \"LD\": {\n",
      "                        \"4\": \"B\",\n",
      "                        \"other\": {\n",
      "                          \"LD\": {\n",
      "                            \"5\": \"L\",\n",
      "                            \"other\": \"R\"\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": \"R\"\n",
      "                  }\n",
      "                },\n",
      "                \"other\": {\n",
      "                  \"LD\": {\n",
      "                    \"1\": {\n",
      "                      \"RD\": {\n",
      "                        \"2\": {\n",
      "                          \"LW\": {\n",
      "                            \"4\": \"B\",\n",
      "                            \"other\": \"R\"\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": \"R\"\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": {\n",
      "                      \"LD\": {\n",
      "                        \"2\": {\n",
      "                          \"RD\": {\n",
      "                            \"2\": {\n",
      "                              \"LW\": {\n",
      "                                \"2\": \"B\",\n",
      "                                \"other\": \"L\"\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": {\n",
      "                              \"LW\": {\n",
      "                                \"4\": {\n",
      "                                  \"RD\": {\n",
      "                                    \"3\": \"L\",\n",
      "                                    \"other\": {\n",
      "                                      \"RD\": {\n",
      "                                        \"4\": \"B\",\n",
      "                                        \"other\": \"R\"\n",
      "                                      }\n",
      "                                    }\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": {\n",
      "                                  \"RD\": {\n",
      "                                    \"3\": {\n",
      "                                      \"LW\": {\n",
      "                                        \"2\": \"R\",\n",
      "                                        \"other\": \"B\"\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": \"R\"\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": {\n",
      "                          \"LW\": {\n",
      "                            \"2\": {\n",
      "                              \"RD\": {\n",
      "                                \"5\": {\n",
      "                                  \"LD\": {\n",
      "                                    \"5\": \"B\",\n",
      "                                    \"other\": \"R\"\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": {\n",
      "                                  \"LD\": {\n",
      "                                    \"3\": {\n",
      "                                      \"RD\": {\n",
      "                                        \"2\": \"L\",\n",
      "                                        \"other\": {\n",
      "                                          \"RD\": {\n",
      "                                            \"3\": \"B\",\n",
      "                                            \"other\": \"R\"\n",
      "                                          }\n",
      "                                        }\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": {\n",
      "                                      \"RD\": {\n",
      "                                        \"4\": {\n",
      "                                          \"LD\": {\n",
      "                                            \"4\": \"B\",\n",
      "                                            \"other\": \"L\"\n",
      "                                          }\n",
      "                                        },\n",
      "                                        \"other\": \"L\"\n",
      "                                      }\n",
      "                                    }\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": {\n",
      "                              \"RD\": {\n",
      "                                \"5\": {\n",
      "                                  \"LD\": {\n",
      "                                    \"3\": {\n",
      "                                      \"LW\": {\n",
      "                                        \"3\": \"R\",\n",
      "                                        \"other\": \"L\"\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": \"L\"\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": \"L\"\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"other\": {\n",
      "              \"RD\": {\n",
      "                \"2\": {\n",
      "                  \"LW\": {\n",
      "                    \"1\": \"R\",\n",
      "                    \"other\": {\n",
      "                      \"LD\": {\n",
      "                        \"1\": \"R\",\n",
      "                        \"other\": {\n",
      "                          \"LD\": {\n",
      "                            \"2\": {\n",
      "                              \"LW\": {\n",
      "                                \"4\": {\n",
      "                                  \"RW\": {\n",
      "                                    \"3\": \"L\",\n",
      "                                    \"other\": {\n",
      "                                      \"RW\": {\n",
      "                                        \"4\": \"B\",\n",
      "                                        \"other\": \"R\"\n",
      "                                      }\n",
      "                                    }\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": {\n",
      "                                  \"RW\": {\n",
      "                                    \"3\": {\n",
      "                                      \"LW\": {\n",
      "                                        \"2\": \"R\",\n",
      "                                        \"other\": \"B\"\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": \"R\"\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": {\n",
      "                              \"LW\": {\n",
      "                                \"2\": {\n",
      "                                  \"LD\": {\n",
      "                                    \"3\": {\n",
      "                                      \"RW\": {\n",
      "                                        \"3\": \"B\",\n",
      "                                        \"other\": \"R\"\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": {\n",
      "                                      \"RW\": {\n",
      "                                        \"3\": \"L\",\n",
      "                                        \"other\": {\n",
      "                                          \"LD\": {\n",
      "                                            \"4\": {\n",
      "                                              \"RW\": {\n",
      "                                                \"4\": \"B\",\n",
      "                                                \"other\": \"R\"\n",
      "                                              }\n",
      "                                            },\n",
      "                                            \"other\": {\n",
      "                                              \"RW\": {\n",
      "                                                \"4\": \"L\",\n",
      "                                                \"other\": \"B\"\n",
      "                                              }\n",
      "                                            }\n",
      "                                          }\n",
      "                                        }\n",
      "                                      }\n",
      "                                    }\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": {\n",
      "                                  \"RW\": {\n",
      "                                    \"5\": {\n",
      "                                      \"LD\": {\n",
      "                                        \"3\": \"R\",\n",
      "                                        \"other\": \"L\"\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": \"L\"\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                },\n",
      "                \"other\": {\n",
      "                  \"LW\": {\n",
      "                    \"3\": {\n",
      "                      \"LD\": {\n",
      "                        \"5\": {\n",
      "                          \"RW\": {\n",
      "                            \"3\": {\n",
      "                              \"RD\": {\n",
      "                                \"5\": \"B\",\n",
      "                                \"other\": \"L\"\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": {\n",
      "                              \"RD\": {\n",
      "                                \"3\": {\n",
      "                                  \"RW\": {\n",
      "                                    \"4\": \"L\",\n",
      "                                    \"other\": \"B\"\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": \"R\"\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": {\n",
      "                          \"LD\": {\n",
      "                            \"4\": {\n",
      "                              \"RW\": {\n",
      "                                \"3\": {\n",
      "                                  \"RD\": {\n",
      "                                    \"3\": \"L\",\n",
      "                                    \"other\": {\n",
      "                                      \"RD\": {\n",
      "                                        \"4\": \"B\",\n",
      "                                        \"other\": \"R\"\n",
      "                                      }\n",
      "                                    }\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": {\n",
      "                                  \"RD\": {\n",
      "                                    \"3\": {\n",
      "                                      \"RW\": {\n",
      "                                        \"4\": \"B\",\n",
      "                                        \"other\": \"R\"\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": \"R\"\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": {\n",
      "                              \"LD\": {\n",
      "                                \"3\": {\n",
      "                                  \"RW\": {\n",
      "                                    \"3\": {\n",
      "                                      \"RD\": {\n",
      "                                        \"3\": \"B\",\n",
      "                                        \"other\": \"R\"\n",
      "                                      }\n",
      "                                    },\n",
      "                                    \"other\": \"R\"\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": \"R\"\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    },\n",
      "                    \"other\": {\n",
      "                      \"LD\": {\n",
      "                        \"5\": {\n",
      "                          \"RW\": {\n",
      "                            \"3\": {\n",
      "                              \"RD\": {\n",
      "                                \"3\": {\n",
      "                                  \"LW\": {\n",
      "                                    \"1\": \"R\",\n",
      "                                    \"other\": \"L\"\n",
      "                                  }\n",
      "                                },\n",
      "                                \"other\": \"R\"\n",
      "                              }\n",
      "                            },\n",
      "                            \"other\": \"R\"\n",
      "                          }\n",
      "                        },\n",
      "                        \"other\": \"R\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 分类树剪枝\n",
    "m = len(data_set)\n",
    "data_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "for i in range(int(m * 0.7)):\n",
    "    data_train.append(data_set[i])\n",
    "    \n",
    "for i in range(int(m * 0.7), m):\n",
    "    X_val.append(data_set[i][0:-1])\n",
    "    y_val.append(data_set[i][-1])\n",
    "\n",
    "classifier_before = generate_cart_classifier(data_train, data_info)\n",
    "#print(json.dumps(eval(str(classifier_before)), sort_keys=True, indent=2))\n",
    "classifier_after = pruning_cart(classifier_before, X_val, y_val, \"classifier\")\n",
    "print(json.dumps(eval(str(classifier_before)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
