{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "决策树是基于树结构来进行决策的，类似人在面临决策问题时一种很自然的处理机制。下图是一个决策树的例子，关于女孩决定是否去相亲：\n",
    "![女孩决定是否去相亲决策树](images/image01.png)\n",
    "\n",
    "一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定序列。使用决策树进行决策的过程就是从根结点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子结点，将叶子结点存放的类别作为决策结果。\n",
    "\n",
    "## 生成决策树\n",
    "\n",
    "决策树算法的伪代码：\n",
    "\n",
    "输入： 训练集 $D = \\{(x_1, y_1),(x_2, y_2),\\cdots,(x_m, y_m)\\}$；\n",
    "     属性集 $A = \\{a_1, a_2,\\cdots,a_m\\}$。\n",
    "\n",
    "过程：函数generate_tree(D, A)\n",
    "\n",
    "```pascal\n",
    "生成结点 node；\n",
    "if D 中样本全属于同一类别C then\n",
    "    将 node 标记为C类叶结点；return\n",
    "end if\n",
    "if A = 空集 then\n",
    "    将 node 标记为叶结点，其类别标记为D中样本数最多的类；return\n",
    "end if\n",
    "从A中选择最优划分属性a；\n",
    "for a 的每一个值 v do\n",
    "    为 node 生成一个分支；令D_v表示D中在a上取值v的样本子集\n",
    "    if D_v 为空 then\n",
    "        将分支结点标记为叶结点，其类别标记为D中样本最多的类；return\n",
    "    else\n",
    "        以 generate_tree(D_v, A \\ {a})为分支结点\n",
    "    end if\n",
    "end for\n",
    "```\n",
    "\n",
    "输出：以 node 为根结点的一棵决策树。\n",
    "\n",
    "\n",
    "在从A中选择最优划分属性a中，根据策略的不同，决策树可以分为ID3、C4.5和CART。\n",
    "\n",
    "\n",
    "首先，我们来定义决策树使用的数据类型：结点。结点分为内部结点和叶子结点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    结点的父类\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_classes_num = None  # 训练集类分布\n",
    "        self.val_classes_num = None  # 验证集类分布\n",
    "        self.data_info = None  # 训练集信息\n",
    "\n",
    "class InternalNode(Node):\n",
    "    \"\"\"\n",
    "    内部结点\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature = None  # 属性名称\n",
    "        self.axis = None  # 属性在训练集中的位置\n",
    "        self.branch = []  # 结点的分支\n",
    "        self.feature_values = [] # 属性的取值，位置与branch对应\n",
    "        \n",
    "    def __str__(self):\n",
    "        print_str = \"{\" + \"\\\"\" + self.feature + \"\\\"\" + \": {\"\n",
    "        for i in range(len(self.branch)):\n",
    "            print_str += str(\"\\\"\" + self.feature_values[i] + \"\\\"\" + \": \" + str(self.branch[i])) + \", \"\n",
    "        print_str = print_str[:-2]\n",
    "        print_str += \"}}\"\n",
    "        return print_str\n",
    "    \n",
    "\n",
    "class LeafNode(Node):\n",
    "    \"\"\"\n",
    "    叶子结点\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.class_label = None  # class label\n",
    "        #self.val_set = []  # 保存验证集样本\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\\"\" + self.class_label + \"\\\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析函数`generate_tree`，我们需要一个函数`get_max_num_class`来获取训练集中数量最多的类，需要函数`split_data_set`来划分训练集，需要函数`get_best_feature`来选择最优划分属性，需要函数`count_values`统计属性或类别标签在训练集中的值的分布。我们接下来实现这4个函数。\n",
    "\n",
    "**任务1：**实现`get_max_num_class`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_num_class(classes_num):\n",
    "    \"\"\"\n",
    "    获取数量最多的类\n",
    "    参数：\n",
    "        classes_num: 类别标签的分布，比如[1,0,2,3]\n",
    "    返回：\n",
    "        数量最多的类的位置\n",
    "    \"\"\"\n",
    "    \n",
    "    cls = None\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,0,2,3]\n",
    "print(get_max_num_class(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务2：**实现`split_data_set`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_set(data_set, axis, value, is_delete_axis=True):\n",
    "    \"\"\"\n",
    "    在训练集的axis列，选择值为value的训练集样本，并返回\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        axis： 属性所在列\n",
    "        value： 属性的所取的值\n",
    "        is_delete_axis: 是否删除第axis列\n",
    "    返回：\n",
    "        划分的子训练集\n",
    "    \"\"\"\n",
    "    sub_data_set = []\n",
    "    ### START CODE HERE ###\n",
    "    for sample in data_set:\n",
    "        if sample[axis] == value:\n",
    "            if is_delete_axis:\n",
    "                # 删除第axis列并把sample保存到sub_data_set中\n",
    "                \n",
    "            else:\n",
    "                # 把sample保存到sub_data_set中\n",
    "                \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return sub_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[1,1],[1,2],[2,3],[2,1],[3,2],[3,3]]\n",
    "print(split_data_set(test, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数`count_values`统计数据集中某一列的值的分布。\n",
    "\n",
    "**任务3：**实现`count_values`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(data_set, axis, values):\n",
    "    \"\"\"\n",
    "    统计values在数据集中第axis列出现的个数\n",
    "    参数：\n",
    "        data_set: 数据集\n",
    "        axis: 位置\n",
    "        values: 统计的值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    value_list = [sample[axis] for sample in data_set]\n",
    "    values_num = []\n",
    "    for i in range(len(values)):\n",
    "        # 统计特征值values【i】出现的次数\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return values_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  [[1,1],[1,2],[2,3],[2,1],[3,2],[3,3]]\n",
    "print(count_values(test, -1, [1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数`get_best_feature`选择最优划分属性，有许多划分策略，现在实现一个简单的策略：选择属性集中第一个属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature(data_set, features, data_info):\n",
    "    \"\"\"\n",
    "    根据某种策略选择最优划分属性\n",
    "    这里选择第一个属性\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        features： 属性集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        选择的属性的位置axis\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务4：**实现`generate_tree`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tree(data_set, features, get_best_feature, data_info):\n",
    "    \"\"\"\n",
    "    生成决策树\n",
    "    参数：\n",
    "        data_set: 训练集，最后一列是类别标签，其他是属性eatures\n",
    "        features: 属性集\n",
    "        get_best_feature: 选择最优划分属性的函数\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        决策树的结点\n",
    "    \"\"\"\n",
    "    assert len(data_set) != 0, \"the size of data set cannot be 0\"\n",
    "    \n",
    "    features = features[:]\n",
    "    \n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    val_classes_num = [0 for i in classes_num]\n",
    "    \n",
    "    class_list = [sample[-1] for sample in data_set]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # 如果训练集的样本都属于同一类，返回类别标记为这个类的叶子结点\n",
    "    if len(set(class_list)) == 1:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = None\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "\n",
    "    # 如果属性集为空，返回类别标记为训练集中数量最多的类的叶子结点\n",
    "    if len(features) == 0:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = None\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "\n",
    "    node = InternalNode()\n",
    "    node.data_info = data_info\n",
    "    node.train_classes_num = classes_num\n",
    "    node.val_classes_num = val_classes_num\n",
    "    # 选择最优划分属性a\n",
    "    axis = None\n",
    "    best_feature = features[axis]\n",
    "    node.feature = best_feature\n",
    "    node.axis = data_info[\"features\"].index(best_feature)\n",
    "    del features[axis]\n",
    "\n",
    "    feature_values = data_info[\"features_values\"][best_feature]\n",
    "    node.feature_values = feature_values\n",
    "    # 根据属性a取值划分训练集\n",
    "    for feature_value in feature_values:\n",
    "        sub_data_set = None\n",
    "        if len(sub_data_set) == 0:\n",
    "            sub_node = LeafNode()\n",
    "            sub_node.data_info = data_info\n",
    "            sub_node.class_label = data_info[\"classes\"][get_max_num_class(classes_num)]\n",
    "            sub_node.train_classes_num = count_values(sub_data_set, -1, data_info[\"classes\"])\n",
    "            sub_node.val_classes_num = val_classes_num\n",
    "            node.branch.append(sub_node)\n",
    "        else:\n",
    "            # 为子集生成决策树\n",
    "            None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经完成了函数`generate_tree`，让我们导入数据并生成一个决策树。我们使用的数据来自[http://archive.ics.uci.edu/ml/datasets/Balance+Scale](http://archive.ics.uci.edu/ml/datasets/Balance+Scale)。该数据用来模拟心理学实验结果，它的一条记录为：\n",
    "\n",
    "1. Class Name: 3 (L, B, R) \n",
    "2. Left-Weight: 5 (1, 2, 3, 4, 5) \n",
    "3. Left-Distance: 5 (1, 2, 3, 4, 5) \n",
    "4. Right-Weight: 5 (1, 2, 3, 4, 5) \n",
    "5. Right-Distance: 5 (1, 2, 3, 4, 5)\n",
    "\n",
    "分类的正确方法是选择$\\text{Left-Weight} \\times \\text{Left-Distance}$和$\\text{Right-Weight} \\times \\text{Right-Distance}$中的较大者。如果相等，那么类别为B。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"balance-scale.data\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "row_list = content.splitlines()\n",
    "data_set = [row.split(\",\") for row in row_list if row.strip()]\n",
    "# 调整类别标签和属性的位置\n",
    "for sample in data_set:\n",
    "    class_label = sample.pop(0)\n",
    "    sample.append(class_label)\n",
    "\n",
    "features = [\"LW\", \"LD\", \"RW\", \"RD\"]\n",
    "feature_values = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "features_values = {\"LW\": feature_values, \"LD\": feature_values, \"RW\": feature_values, \"RD\": feature_values}\n",
    "classes = [\"L\", \"B\",  \"R\"]\n",
    "data_info = {\"features\": features, \"features_values\": features_values, \"classes\": classes}\n",
    "\n",
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "    \n",
    "decision_tree = generate_tree(data_set[:10], features, get_best_feature, data_info)\n",
    "print(json.dumps(eval(str(decision_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，先前实现的`get_best_feature`使用的选择最优划分属性策略不好，生成的决策树太复杂。因此，需要选择好的策略。\n",
    "\n",
    "## ID3\n",
    "\n",
    "熵(entropy)：信息量大小的度量，即表示随机变量不确定性的度量。\n",
    "\n",
    "事件$a_i$的信息量$I(a_i)$可如下度量：$I(a_i) = -p(a_i) \\log p(a_i)$，这里$p(a_i)$为事件$a_i$发生的概率。\n",
    "\n",
    "假设有n个互不相容的事件$a_i, \\cdots, a_n$，它们中有且仅有一个发生，则其平均的信息量（熵）可如下度量：\n",
    "$$\n",
    "I(a_i, \\cdots, a_n) = \\sum_{i} I(a_i) = - \\sum_{i} p(a_i) \\log p(a_i)\n",
    "$$\n",
    "\n",
    "假设当前训练集D中第k类样本比例为$p_k$，对应的信息熵为：\n",
    "$$\n",
    "\\text{Ent}(D) = - \\sum_k p_k \\log p_k\n",
    "$$\n",
    "\n",
    "$\\text{Ent}(D)$越小，表示数据越有序，纯度越高，分类效果越好。\n",
    "\n",
    "假设某离散属性a有V个可能值，若采用该属性对样本集来划分，则会产生V个分支，每个分支节点包含的数据记为$D^v$。用属性a对训练集D进行划分，获得的信息增益为：\n",
    "$$\n",
    "\\text{Gain}(D, a) = \\text{Ent}(D) - \\sum_v \\frac{|D^v|}{|D|} \\text{Ent}(D^v)\n",
    "$$\n",
    "\n",
    "ID3选择具有最大信息增益的属性来划分：$a* = \\arg \\underset{a}{\\max} \\text{Gain}(D, a)$。\n",
    "\n",
    "接下来实现ID3版本的选择最优划分属性函数`get_best_feature_id3`。首先实现计算训练集信息熵函数`compute_entropy`。\n",
    "\n",
    "**任务5：**实现`compute_entropy`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(values_num):\n",
    "    \"\"\"\n",
    "    根据变量的分布values_num计算信息熵\n",
    "    参数：\n",
    "        values_num: 变量的分布\n",
    "    返回：\n",
    "        信息熵\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    m = None\n",
    "    entropy = 0\n",
    "    for value_num in values_num:\n",
    "        if value_num == 0:  # 不能计算 log(0)\n",
    "            continue\n",
    "        prob = None\n",
    "        entropy -= \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3]\n",
    "print(compute_entropy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务6：**实现`get_best_feature_id3`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_id3(data_set, features, data_info):\n",
    "    \"\"\"\n",
    "    根据信息增益选择划分属性\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        features： 属性集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        选择的属性的位置axis\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # 计算训练集的信息熵\n",
    "    m = len(data_set)\n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    entropy = None\n",
    "\n",
    "    gain = 0\n",
    "    best_axis = 0\n",
    "    # 计算按照属性a的取值划分训练集后的信息熵\n",
    "    for i in range(len(features)):\n",
    "        split_entropy = 0\n",
    "        feature_values = data_info[\"features_values\"][features[i]]\n",
    "        for feature_value in feature_values:\n",
    "            sub_data_set = None\n",
    "            sub_classes_num = None\n",
    "            split_entropy += None\n",
    "        if entropy - split_entropy > gain:\n",
    "            gain = entropy - split_entropy\n",
    "            best_axis = i\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return best_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "    \n",
    "id3_tree = generate_tree(data_set[:10], features, get_best_feature_id3, data_info)\n",
    "print(json.dumps(eval(str(id3_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，ID3生成的决策树比之前生成的决策树要简单得多。现在可以使用生成的决策树对新样本进行分类。\n",
    "\n",
    "**任务7：**实现决策树的推理函数`inference`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(tree, sample, is_add_to_node=False):\n",
    "    \"\"\"\n",
    "    对新样本进行分类\n",
    "    参数：\n",
    "        sample: 新样本\n",
    "        is_add_to_node: 是否把新样本类别信息放到结点中\n",
    "    返回：\n",
    "        新样本的分类结果\n",
    "    \"\"\"\n",
    "    node = tree\n",
    "    classes = tree.data_info[\"classes\"]\n",
    "    if is_add_to_node:\n",
    "        node.val_classes_num[classes.index(sample[-1])] += 1\n",
    "        \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    while type(node) == InternalNode:\n",
    "        for i in range(len(node.feature_values)):\n",
    "            if None:\n",
    "                node = None\n",
    "                break\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node.class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = inference(id3_tree, data_set[0], data_info)\n",
    "print(class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5\n",
    "\n",
    "ID3算法的基本思想是以信息增益选择属性，实际应用中会对可能取值数目较多的属性有所偏好。例如，如果对每个训练样本进行编号，并将编号作为属性，其信息增益最大，但显然该属性不能作为分类依据。因此，C4.5算法使用信息增益比来选择划分属性。\n",
    "\n",
    "信息增益比公式如下：\n",
    "$$\n",
    "\\text{Gain_ratio}(D, a) = \\frac{\\text{Gain}(D,a)}{\\text{IV}(a)}\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "\\text{IV}(a) = - \\sum_{v=1}^V \\frac{|D^v|}{|D|} \\log \\frac{|D^v|}{|D|}\n",
    "$$\n",
    "是属性a的固定值(intrinsic value)，a可能的取值越多，$\\text{IV}(a)$通常也越大。\n",
    "\n",
    "接下来实现C4.5版本的选择最优划分属性函数`get_best_feature_c45`。在这里不设置信息增益比的阈值$\\epsilon$，直接选择具有最大信息增益比的属性。\n",
    "\n",
    "**任务8：**实现`get_best_feature_c45`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_c45(data_set, features, data_info):\n",
    "    \"\"\"\n",
    "    根据信息增益率选择划分属性\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        features： 属性集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        选择的属性的位置axis\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # 计算训练集的信息熵\n",
    "    m = len(data_set)\n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    entropy = None\n",
    "\n",
    "    gain_ratio = 0\n",
    "    best_axis = 0\n",
    "    # 计算按照属性a的取值划分训练集后的信息熵\n",
    "    for i in range(len(features)):\n",
    "        split_entropy = 0\n",
    "        gain = 0\n",
    "        feature_values_num = count_values(data_set, i, data_info[\"features_values\"][features[i]])\n",
    "        # iv + 1e-7 防止分母为0\n",
    "        iv = None + 1e-7\n",
    "        feature_values = data_info[\"features_values\"][features[i]]\n",
    "        for feature_value in feature_values:\n",
    "            sub_data_set = None\n",
    "            sub_classes_num = None\n",
    "            split_entropy += None\n",
    "        gain = entropy - split_entropy\n",
    "        if gain / iv > gain_ratio:\n",
    "            gain_ratio = gain / iv\n",
    "            best_axis = i\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return best_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "    \n",
    "c45_tree = generate_tree(data_set[:10], features, get_best_feature_c45, data_info)\n",
    "print(json.dumps(eval(str(c45_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为数据简单，C4.5生成的决策树和ID3生成的决策树一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "通常生成的决策树复杂度非常高，过度拟合训练数据，导致过拟合，决策树的剪枝减少决策树的规模，是处理过拟合问题的主要手段。\n",
    "\n",
    "通过极小化决策树整体的 Cost Complexity 函数实现剪枝\n",
    "$$\n",
    "\\text{CC}(T) = \\text{Err}(T) + \\lambda R(T)\n",
    "$$\n",
    "其中$\\text{Err}(T)$为树T的错误，$R(T)$为正则化项，描述树的复杂度（如树节点的个数），$\\lambda \\ge 0$为正则化参数。\n",
    "\n",
    "设树T的叶结点个数为$|T|$，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$。叶结点t上的信息熵：\n",
    "$$\n",
    "H_t(T) = - \\sum_{k=1}^K \\frac{N_{tk}}{N_t} \\log \\frac{N_{tk}}{N_t} \n",
    "$$\n",
    "$\\text{Err}(T)$为\n",
    "$$\n",
    "\\text{Err}(T) = \\sum_{i=1}^{|T|} N_t H_t(T) = - \\sum_{i=1}^{|T|} \\sum_{k=1}^K N_{tk} \\log \\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "则\n",
    "$$\n",
    "\\text{CC}(T) = - \\sum_{i=1}^{|T|} \\sum_{k=1}^K N_{tk} N_{tk} \\log \\frac{N_{tk}}{N_t}  + \\lambda |T|\n",
    "$$\n",
    "\n",
    "决策树的剪枝算法：\n",
    "\n",
    "- 输入：算法产生的整个决策树T，正则化参数$\\lambda$\n",
    "- 输出：修剪后的决策树T\n",
    "    - Step 1: 计算每个结点的代价\n",
    "    - Step 2: 递归地从树的叶结点向上回溯\n",
    "- 设一组叶结点回溯到其父结点之前与之后的代价分别为：$\\text{CC}(T_B)$和$\\text{CC}(T_A)$，若$\\text{CC}(T_A) \\le \\text{CC}(T_B)$，则剪枝，返回Step 2，直到不能继续为止。\n",
    "- 注意是根据验证集上的代价来决定是否剪枝。\n",
    "\n",
    "**任务9：**下面我们来实现计算 Cost Complexity 的函数`compute_tree_cc`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tree_cc(tree, lambd=1):\n",
    "    \"\"\"\n",
    "    计算决策树的 Cost Complexity 值\n",
    "    参数：\n",
    "        tree: 决策树\n",
    "        lambd: 正则化参数\n",
    "    返回：\n",
    "        决策树的 Cost Complexity 值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    err = 0\n",
    "    leaf_count = 0\n",
    "    node_list = [tree]\n",
    "    while len(node_list) != 0:\n",
    "        node = node_list.pop(0)\n",
    "        if type(node) == InternalNode:\n",
    "            for i in node.branch:\n",
    "                node_list.append(i)\n",
    "        else:\n",
    "            Nt = sum(node.val_classes_num)\n",
    "            ent = None\n",
    "            err += None\n",
    "            leaf_count += 1\n",
    "    \n",
    "    return None\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_tree_cc(c45_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务10：**实现决策树的剪枝函数`pruning`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(tree, p, node, lambd = 1):\n",
    "    \"\"\"\n",
    "    剪枝决策树\n",
    "    参数：\n",
    "        tree: 决策树\n",
    "        p: 父结点\n",
    "        node: 正在遍历的结点\n",
    "        lambd: 正则化参数\n",
    "    返回：\n",
    "        处理后的结点\n",
    "    \"\"\"\n",
    "    if type(node) == LeafNode:\n",
    "        return node\n",
    "    # 后序遍历\n",
    "    for i in range(len(node.branch)):\n",
    "        node.branch[i] = pruning(tree, node, node.branch[i], lambd)\n",
    "    \n",
    "    can_pruning = True\n",
    "    for i in node.branch:\n",
    "        if type(i) == InternalNode:\n",
    "            can_pruning = False\n",
    "            break\n",
    "    \n",
    "    if can_pruning:\n",
    "        CC_before = compute_tree_cc(tree, lambd)\n",
    "        CC_after = None\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # 内部结点转叶子结点\n",
    "        leaf_node = LeafNode()\n",
    "        leaf_node.train_classes_num = node.train_classes_num\n",
    "        leaf_node.val_classes_num = node.val_classes_num\n",
    "        leaf_node.data_info = node.data_info\n",
    "        classes = node.data_info[\"classes\"]\n",
    "        leaf_node.class_label = None\n",
    "        \n",
    "        if tree == node:\n",
    "            CC_after = None\n",
    "        else:\n",
    "            index = p.branch.index(node)\n",
    "            p.branch[index] = leaf_node\n",
    "            CC_after = None\n",
    "            \n",
    "        if CC_after <= CC_before:\n",
    "            return leaf_node\n",
    "        else:\n",
    "            return node\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    else:\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "m = len(data_set)\n",
    "m_train = int(m*0.7)\n",
    "m_val = m - m_train\n",
    "data_train = data_set[:m_train]\n",
    "data_val = data_set[m_train:]\n",
    "classes_label_val = [sample[-1] for sample in data_val]\n",
    "\n",
    "c45_tree_before = generate_tree(data_train, features, get_best_feature_c45, data_info)\n",
    "print(\"剪枝前：\")\n",
    "#print(json.dumps(eval(str(c45_tree_before)), sort_keys=True, indent=2))\n",
    "\n",
    "predict = []\n",
    "for sample in data_val:\n",
    "    predict.append(inference(c45_tree_before, sample, is_add_to_node=True))\n",
    "\n",
    "is_same = [1 if classes_label_val[i] == predict[i] else 0 for i in range(m_val)]\n",
    "accuracy = sum(is_same) / m_val\n",
    "print(\"the accuracy before pruning is %.3f\" % accuracy)\n",
    "\n",
    "# 剪枝\n",
    "c45_tree_after = pruning(c45_tree_before, None, c45_tree_before, lambd=1)\n",
    "print(\"剪枝后：\")\n",
    "#print(json.dumps(eval(str(c45_tree_after)), sort_keys=True, indent=2))\n",
    "\n",
    "predict = []\n",
    "for sample in data_val:\n",
    "    predict.append(inference(c45_tree_after, sample, is_add_to_node=False))\n",
    "\n",
    "is_same = [1 if classes_label_val[i] == predict[i] else 0 for i in range(m_val)]\n",
    "accuracy = sum(is_same) / m_val\n",
    "print(\"the accuracy before pruning is %.3f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "\n",
    "CART(Classification and Regression Tree)算法是目前决策树算法中最为成熟的一类算法，它既可以用于分类（CART决策树），又可以用于预测（CART回归树）。\n",
    "\n",
    "CART决策树与ID3、C4.5的不同之处在于CART决策树是一棵二叉树，使用基尼系数（Gini Index）来选择划分属性。\n",
    "\n",
    "基尼值的计算公式为：\n",
    "$$\n",
    "\\text{Gini}(D) = \\sum_{k=1}^{K} \\sum_{k' \\not = k} p_k p_{k'} = 1 - \\sum_{k=1}^K p_k^2\n",
    "$$\n",
    "\n",
    "直观上，基尼值反应了从数据集中任选2个样本，其类别不一致的概率，其值越小，纯度越高。\n",
    "\n",
    "基尼系数的计算公式为：\n",
    "$$\n",
    "\\text{Gini_index}(D, a) = \\sum_{v=1}^V \\frac{|D^v|}{|D|} \\text{Gini}(D^v)\n",
    "$$\n",
    "\n",
    "CART决策树使用的是离散值的属性，如果样本的属性是连续型的，需要使用离散化方法把连续属性离散化。\n",
    "\n",
    "CART决策树生成算法：\n",
    "\n",
    "- 输入：训练数据集D，迭代终止条件\n",
    "- 输出：CART决策树\n",
    "- 从根结点开始，递归对每个结点进行以下操作构造二叉树\n",
    "    - 设结点数据集为D，对每个特征A，对其每个值a，根据样本点对$A=a$的测试为是或否，将D分为$D^-$,$D^+$，计算$A=a$的基尼系数\n",
    "    - 在所有的特征A以及所有可能的切分点a中，选择基尼指数最小的特征和切分点，将数据集分配到两个子结点中\n",
    "    - 对两个子结点递归调用上面两个步骤，最终生成CART决策树\n",
    "    \n",
    "下面开始实现CART决策树，我们需要`compute_gini`计算基尼值，`compute_gini_index`计算基尼系数。\n",
    "\n",
    "**任务11：**实现`compute_gini`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gini(class_num):\n",
    "    \"\"\"\n",
    "    计算基尼值\n",
    "    参数：\n",
    "        class_num: 类别的分布信息\n",
    "    返回：\n",
    "        基尼值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    gini = 0\n",
    "    m = None\n",
    "    for i in class_num:\n",
    "        prob = None\n",
    "        gini += None\n",
    "    return None\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1, 2, 3]\n",
    "print(compute_gini(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务12：**实现`compute_gini_index`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gini_index(data_set, axis, value, data_info):\n",
    "    \"\"\"\n",
    "    计算(D, a)基尼系数\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        axis: 属性位置\n",
    "        value: 选取的属性值\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        基尼系数\n",
    "    \"\"\"\n",
    "    m = len(data_set)\n",
    "    \n",
    "    classes = data_info[\"classes\"]\n",
    "    \n",
    "    feature = data_info[\"features\"][axis]\n",
    "    feature_values = data_info[\"features_values\"][feature]\n",
    "    \n",
    "    values_num = count_values(data_set, axis, feature_values)\n",
    "    \n",
    "    value_index = feature_values.index(value)\n",
    "    value_count = values_num[value_index]\n",
    "    others_count = m - value_count\n",
    "    ### START CODE HERE ###\n",
    "    # compute gini of value\n",
    "    sub_data_set = None\n",
    "    value_gini = None\n",
    "    \n",
    "    # compute gini of others\n",
    "    sub_data_set = []\n",
    "    for feature_value in feature_values:\n",
    "        if feature_value == value:\n",
    "            continue\n",
    "        sub_data_set.extend(split_data_set(data_set, axis, feature_value))\n",
    "    others_gini = None\n",
    "    \n",
    "    return None\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 2],\n",
    "    [2, 1, 1],\n",
    "    [2, 2, 2]\n",
    "]\n",
    "\n",
    "test_data_info = {\n",
    "    \"features\": [1, 2],\n",
    "    \"features_values\": {1: [1, 2], 2: [1, 2]},\n",
    "    \"classes\": [1, 2]\n",
    "}\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(1, 3):\n",
    "        print(compute_gini_index(test, i, j, test_data_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数`get_best_feature_cart_decision`用于获取基尼系数最小的属性和切分点。\n",
    "\n",
    "**任务13：**实现`get_best_feature_cart_decision`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_cart_decision(data_set, data_info):\n",
    "    \"\"\"\n",
    "    根据基尼系数选择最优划分属性和切分点\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        data_info: 保存完整属性集的信息，{features:[], features_values={feature1:[], ... }, classes:[]}\n",
    "    返回：\n",
    "        (axis, vlaue): 属性位置，切分点值\n",
    "    \"\"\"\n",
    "    m = len(data_set)\n",
    "    \n",
    "    lowest_gini_index = 1\n",
    "    axis = 0\n",
    "    value = None\n",
    "    \n",
    "    classes = data_info[\"classes\"]\n",
    "    \n",
    "    features = data_info[\"features\"]\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(len(features)):\n",
    "        feature = None\n",
    "        gini_index = 0\n",
    "        \n",
    "        feature_values = None\n",
    "        values_num = count_values(data_set, i, feature_values)\n",
    "        # 该属性在data_set中只有一个值，不能把数据分成两部分\n",
    "        if m in values_num:\n",
    "            continue\n",
    "        \n",
    "        for j in range(len(feature_values)):\n",
    "            if values_num[j] == 0:\n",
    "                continue\n",
    "            gini_index = None\n",
    "            \n",
    "            if gini_index < lowest_gini_index:\n",
    "                lowest_gini_index = gini_index\n",
    "                axis = i\n",
    "                value = feature_values[j]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return (axis, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 2],\n",
    "    [2, 1, 1],\n",
    "    [2, 2, 2]\n",
    "]\n",
    "\n",
    "test_data_info = {\n",
    "    \"features\": [1, 2],\n",
    "    \"features_values\": {1: [1, 2], 2: [1, 2]},\n",
    "    \"classes\": [1, 2]\n",
    "}\n",
    "\n",
    "print(get_best_feature_cart_decision(test, test_data_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务14：**实现`generate_cart_classifier`函数，生成CART决策树。迭代终止条件有许多种，比如限制树的最大深度、限制叶子结点的个数等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cart_classifier(data_set, data_info):\n",
    "    \"\"\"\n",
    "    生成CART决策树\n",
    "    使用默认终止条件\n",
    "    参数：\n",
    "        data_set: 训练集\n",
    "        data_info: 训练集信息\n",
    "    返回：\n",
    "        决策树的结点\n",
    "    \"\"\"\n",
    "    m = len(data_set)\n",
    "    assert m != 0, \"the size of data set cannot be 0\"\n",
    "    \n",
    "    classes_num = count_values(data_set, -1, data_info[\"classes\"])\n",
    "    val_classes_num = [0] * len(classes_num)\n",
    "    \n",
    "    # 如果训练集的样本都属于同一类，返回类别标记为这个类的叶子结点\n",
    "    if m in classes_num:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = data_set[0][-1]\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "    \n",
    "    # 如果训练集的每种属性都取相同的值，返回类别标记为训练集中数量最多的类的叶子结点\n",
    "    is_stop = True\n",
    "    features = data_info[\"features\"]\n",
    "    for feature in features:\n",
    "        feature_values = data_info[\"features_values\"][feature]\n",
    "        values_num = count_values(data_set, features.index(feature), feature_values)\n",
    "        if m not in values_num:\n",
    "            is_stop = False\n",
    "            break\n",
    "    if is_stop:\n",
    "        node = LeafNode()\n",
    "        node.data_info = data_info\n",
    "        node.class_label = data_info[\"classes\"][get_max_num_class(classes_num)]\n",
    "        node.train_classes_num = classes_num\n",
    "        node.val_classes_num = val_classes_num\n",
    "        return node\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    node = InternalNode()\n",
    "    node.data_info = data_info\n",
    "    node.train_classes_num = classes_num\n",
    "    node.val_classes_num = val_classes_num\n",
    "    # 选择使基尼系数最小的属性和切分点\n",
    "    axis, value = None\n",
    "    best_feature = features[axis]\n",
    "    node.feature = best_feature\n",
    "    node.axis = axis\n",
    "    node.feature_values = [value, \"other\"]\n",
    "    \n",
    "    feature_values = data_info[\"features_values\"][best_feature]\n",
    "    # 左子树（value）\n",
    "    sub_data_set_value = None\n",
    "    node.branch.append(None)\n",
    "    \n",
    "    # 右子树（others）\n",
    "    sub_data_set_others = []\n",
    "    for feature_value in feature_values:\n",
    "        if feature_value == value:\n",
    "            continue\n",
    "        sub_data_set_others.extend(None)\n",
    "    node.branch.append(None)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练集的前10个样本进行训练\n",
    "for i in data_set[:10]:\n",
    "    print(i)\n",
    "\n",
    "cart_tree = generate_cart_classifier(data_set[:10], data_info)\n",
    "print(json.dumps(eval(str(cart_tree)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务15：**实现`cart_classifier_inference`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_classifier_inference(tree, sample):\n",
    "    \"\"\"\n",
    "    对sample进行预测\n",
    "    参数：\n",
    "        tree: 分类树 \n",
    "        sampel: 样本\n",
    "    返回：\n",
    "        y_predict: 预测结果\n",
    "    \"\"\"\n",
    "    node = tree\n",
    "    ### START CODE HERE ###\n",
    "    while type(node) == InternalNode:\n",
    "        if sample[node.axis] == None:\n",
    "            node = None\n",
    "        else:\n",
    "            node = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node.class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cart_classifier_inference(cart_tree, data_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART回归树\n",
    "\n",
    "一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元$R_1, R_2, \\cdots, R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M} c_m I(x \\in R_m)\n",
    "$$\n",
    "\n",
    "当输入空间的划分确定时，可以用平方误差$\\sum_{x_i \\in R_m} (y_i - f(x_i))^2$来表示回归树对与训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值，即\n",
    "$$\n",
    "\\hat{c}_m = \\text{avg} (y_i | x_i \\in R_m)\n",
    "$$\n",
    "\n",
    "对输入空间的划分方法如下。选择第j个变量$x^{(j)}$和它取的值s，作为切分变量和切分点，并定义两个区域：\n",
    "$$\n",
    "R_1(j,s) = \\{ x | x^{(j)} \\le s \\} \\text{ 和 } R_1(j,s) = \\{ x | x^{(j)} \\gt s \\}\n",
    "$$\n",
    "然后寻找最优切分变量j和最优切分点s。具体地，求解\n",
    "$$\n",
    "\\underset{j,s}{\\min} \\left [ \\underset{c_1}{\\min} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\underset{c_2}{\\min} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2 \\right ]\n",
    "$$\n",
    "对固定输入变量j可以找到最优切分点s。\n",
    "$$\n",
    "\\hat{c}_1 = \\text{avg}(y_i | x_i \\in R_1(j, s)) \\text{ 和 } \\hat{c}_2 = \\text{avg}(y_i | x_i \\in R_2(j, s))\n",
    "$$\n",
    "遍历所有输入变量，找到最优的切分变量j，构成一对$(j,s)$。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树。\n",
    "\n",
    "最小二乘回归树算法：  \n",
    "输入：训练数据集D；  \n",
    "输出：回归树$f(x)$。  \n",
    "在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：  \n",
    "（1）选择最优切分变量j和切分点s，求解：\n",
    "$$\n",
    "\\underset{j,s}{\\min} \\left [ \\underset{c_1}{\\min} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\underset{c_2}{\\min} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2 \\right ]\n",
    "$$\n",
    "遍历变量j，对固定的切分变量j扫描切分点s，选择使上式达到最小值的对$(j,s)$。  \n",
    "（2）用选定的对$(j,s)$划分区域并决定相应的输出值：\n",
    "$$\n",
    "R_1(j,s) = \\{ x | x^{(j)} \\le s \\}, R_1(j,s) = \\{ x | x^{(j)} \\gt s \\} \\\\\n",
    "\\hat{c}_m = \\frac{1}{N_m} \\sum_{x_i \\in R_m(j,s)} y_i, x \\in R_m, m=1,2\n",
    "$$\n",
    "（3）继续对两个子区域调用步骤（1）（2），直至满足停止条件。  \n",
    "（4）将输入空间划分为M个区域$R_1, R_2, \\cdots, R_M$，生成决策树：\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M} \\hat{c}_m I(x \\in R_m)\n",
    "$$\n",
    "\n",
    "下面开始实现CART回归树。首先定义回归树用到的数据类型，因为之前的数据类型Node不适合CART回归树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNode(object):\n",
    "    \"\"\"\n",
    "    CART回归树使用的结点类型\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = \"leaf\"  # 结点类型m，\"leaf\": 叶子结点，\"internal\": 内部结点\n",
    "        self.y = None\n",
    "        self.splitting_variable = None  # 切分变量，特征位置\n",
    "        self.splitting_point = None  # 切分点\n",
    "        self.predict = None  # 输出值\n",
    "        self.left = None  # 左结点\n",
    "        self.right = None  # 右结点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务16：**实现分割训练集的函数`split_X`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(X, y, j, s):\n",
    "    \"\"\"\n",
    "    根据切分变量j和切分点s划分训练集X，y\n",
    "    参数：\n",
    "        X: 训练集\n",
    "        y: 训练集标签\n",
    "        j: 切分变量\n",
    "        s: 切分点\n",
    "    返回:\n",
    "        ((X_1, y_1),(X_2, y_2)): 划分后的训练集\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    indicate = X[:, j] <= s\n",
    "    X_1 = None\n",
    "    y_1 = None\n",
    "    \n",
    "    X_2 = None\n",
    "    y_2 = None\n",
    "    \n",
    "    return ((X_1, y_1), (X_2, y_2))\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.arange(12).reshape(3, 4)\n",
    "test_y = np.array([i for i in range(3)])\n",
    "print(test_X)\n",
    "print(test_y)\n",
    "print(split_X(test_X, test_y, 1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务17：**实现选择最优切分变量j和切分点s的函数`get_splitting_variable_and_point`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitting_variable_and_point(X, y):\n",
    "    \"\"\"\n",
    "    选择最优切分变量j和切分点s\n",
    "    参数：\n",
    "        X: 训练集\n",
    "        y: 训练标签\n",
    "    返回：\n",
    "        (j, s): 最优切分变量j和切分点s\n",
    "    \"\"\"\n",
    "    lowest_loss = sys.maxsize\n",
    "    splitting_variable = 0\n",
    "    splitting_point = 1\n",
    "    m, n = X.shape\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    for j in range(n):\n",
    "        values = list(set(X[:, j]))\n",
    "        values.sort()\n",
    "        for s in range(len(values) - 1):\n",
    "            ((X_1, y_1), (X_2, y_2)) = None\n",
    "            c1 = None\n",
    "            c2 = None\n",
    "            loss1 = None\n",
    "            loss2 = None\n",
    "            loss = None\n",
    "            if loss < lowest_loss:\n",
    "                lowest_loss = loss\n",
    "                splitting_variable = j\n",
    "                splitting_point = values[s]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return (splitting_variable, splitting_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([[1],[2],[3],[4],[5]])\n",
    "test_y = np.array([1,1,1,2,2])\n",
    "\n",
    "print(get_splitting_variable_and_point(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务18：**实现CART回归树的生成函数`generate_cart_regression`，终止条件是决策树达到树的最大深度max_depth。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cart_regressor(X, y, depth, max_depth):\n",
    "    \"\"\"\n",
    "    生成CART回归树。\n",
    "    参数：\n",
    "        X: 训练集\n",
    "        y: 训练集标签\n",
    "        depth: 返回结点的深度\n",
    "        max_depth: 树的最大深度\n",
    "    \"\"\"\n",
    "    node = RNode()\n",
    "    node.y = y\n",
    "    # 该结点的最优输出是y的均值\n",
    "    node.predict = np.mean(y)\n",
    "    m, n = X.shape\n",
    "    # 达到最大深度，生成叶子结点\n",
    "    if depth == max_depth:\n",
    "        node.type = \"leaf\"\n",
    "        return node\n",
    "    \n",
    "    y_vales = set(y[:,0])\n",
    "    # 所有y值都相同\n",
    "    if len(y_vales) == 1:\n",
    "        node.type = \"leaf\"\n",
    "        return node\n",
    "    \n",
    "    # 如果训练集的每种属性都取相同的值\n",
    "    is_stop = True\n",
    "    for i in range(n):\n",
    "        feature_values = set(X[:, i])\n",
    "        if len(set(feature_values)) != 1:\n",
    "            is_stop = False\n",
    "            break\n",
    "    if is_stop:\n",
    "        node.type = \"leaf\"\n",
    "        return node\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    node.type = \"internal\"\n",
    "    j, s = None\n",
    "    node.splitting_variable = j\n",
    "    node.splitting_point = s\n",
    "    ((X_1, y_1), (X_2, y_2)) = None\n",
    "    node.left = None\n",
    "    node.right = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务19：**实现`cart_regressor_inference`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_regressor_inference(cart_regressor, x):\n",
    "    \"\"\"\n",
    "    对X进行预测\n",
    "    参数：\n",
    "        cart_regressor: 回归树\n",
    "        X: 预测集\n",
    "    返回：\n",
    "        y_predict: 预测结果\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    node = cart_regressor\n",
    "    while node.type == \"internal\":\n",
    "        if None:\n",
    "            node = None\n",
    "        else:\n",
    "            node = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return node.predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用CART回归树模拟sin函数\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.linspace(-5, 5, 200)\n",
    "y = np.sin(X)\n",
    "y = y + np.random.rand(1, len(y)) * 1.5 # 加入噪声\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "cart_regressor = generate_cart_regressor(X, y, 1, 5)\n",
    "\n",
    "X_train = np.arange(-5, 5, 0.05).reshape(-1, 1)\n",
    "y_predict = np.zeros((X_train.shape[0], 1))\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    y_predict[i, 0] = cart_regressor_inference(cart_regressor, X[i])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y,c=\"k\", label=\"data\")\n",
    "plt.plot(X_train, y_predict, c=\"r\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面不是实验内容，有兴趣可以了解下CART的剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART剪枝\n",
    "\n",
    "CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列${T_0, T_1, \\cdots, T_n}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。\n",
    "\n",
    "定义子树的损失函数为：\n",
    "$$\n",
    "C_{\\alpha}(T) = C(T) + \\alpha |T|\n",
    "$$\n",
    "其中$C(T)$表示对训练数据的预测误差（如基尼值），$|T|$为子树的叶子结点的个数。\n",
    "对T的任意内部结点t，以t为单结点树的损失函数是：\n",
    "$$\n",
    "C_{\\alpha}(t) = C(t) + \\alpha\n",
    "$$\n",
    "以t为根结点的子树$T_t$的损失函数是：\n",
    "$$\n",
    "C_{\\alpha}(T_t) = C(T_t) + \\alpha |T_t|\n",
    "$$\n",
    "\n",
    "CART剪枝算法如下：\n",
    "\n",
    "输入：CART算法生成的决策树$T_0$:  \n",
    "输出：最优决策树$T_{\\alpha}$：  \n",
    "（1）设$k=0$，$T = T_0$  \n",
    "（2）设$\\alpha = +\\infty$  \n",
    "（3）自下而上地对各内部结点t计算$C(T_t)$，$|T_t|$以及\n",
    "$$\n",
    "g(t) = \\frac{C(t) - C(T_t)}{|T_t| - 1} \\\\\n",
    "\\alpha = \\min(\\alpha, g(t))\n",
    "$$\n",
    "这里$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。  \n",
    "（4）对$g(t) = \\alpha$的内部结点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T。  \n",
    "（5）设$k = k+1$，$\\alpha_k = \\alpha$，$T_k = T$。  \n",
    "（6）如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤（3）；否则令$T_k = T_n$。  \n",
    "（7）采用交叉验证法在子树序列$T_0, T_1, \\cdots, T_n$中选取最优子树$T_{\\alpha}$。\n",
    "\n",
    "算法的详细解释请看李航的《统计学习方法》的5.5.2部分内容。\n",
    "\n",
    "回归树的剪枝算法与决策树的剪枝算法类似。我们同时实现CART分类树和回归树的剪枝算法。首先实现计算$C(t)$和$C(T_t)$的函数`compute_node_loss`和`compute_tree_loss`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_loss(node, tree_type):\n",
    "    \"\"\"\n",
    "    计算C(t)的值\n",
    "    参数：\n",
    "        node：内部结点\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回:\n",
    "        C(t)的值\n",
    "    \"\"\"\n",
    "    if tree_type == \"classifier\":\n",
    "        m = sum(node.train_classes_num)\n",
    "        return m * compute_gini(node.train_classes_num)\n",
    "    if tree_type == \"regressor\":\n",
    "        return np.sum(np.power(node.y - node.predict, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tree_loss_and_leaf(root, tree_type):\n",
    "    \"\"\"\n",
    "    计算C(T_t)的值和树的叶子结点个数\n",
    "    参数：\n",
    "        root: 树的根结点\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回：\n",
    "        C(T_t)的值和树的叶子结点个数\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    leaf_node_num = 0\n",
    "    node_list = [root]\n",
    "    while len(node_list) != 0:\n",
    "        node = node_list.pop(0)\n",
    "        if tree_type == \"classifier\":\n",
    "            if type(node) == InternalNode:\n",
    "                node_list.extend(node.branch)\n",
    "            else:\n",
    "                leaf_node_num += 1\n",
    "                loss += compute_gini(node.train_classes_num)\n",
    "        if tree_type == \"regressor\":\n",
    "            if node.type == \"internal\":\n",
    "                node_list.append(node.left)\n",
    "                node_list.append(node.right)\n",
    "            else:\n",
    "                leaf_node_num += 1\n",
    "                loss += np.sum(np.power(node.y - node.predict, 2))\n",
    "                \n",
    "    return (loss, leaf_node_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现`get_optimal_tree`，用于自下而上地对各内部结点t计算$C(T_t)$，$|T_t|$以及\n",
    "$$\n",
    "g(t) = \\frac{C(t) - C(T_t)}{|T_t| - 1} \\\\\n",
    "\\alpha = \\min(\\alpha, g(t))\n",
    "$$\n",
    "这里$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_tree(tree, p, alpha, tree_type):\n",
    "    \"\"\"\n",
    "    对每个alpha区间，找到最优子树\n",
    "    参数：\n",
    "        tree: CART树\n",
    "        p: 父结点\n",
    "        alpha: 目前alpha的值\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回：\n",
    "        (alpha, p, t): 更新后的alpha值， t的父结点p， 选择的内部结点t\n",
    "    \"\"\"\n",
    "    best_alpha = alpha\n",
    "    best_p = None\n",
    "    best_t = None\n",
    "    if tree_type == \"classifier\":\n",
    "        if type(tree.branch[0]) == InternalNode:\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.branch[0], tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        if type(tree.branch[1]) == InternalNode:\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.branch[1], tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        node_loss = compute_node_loss(tree, tree_type)\n",
    "        tree_loss, num = compute_tree_loss_and_leaf(tree, tree_type)\n",
    "        gt = (node_loss - tree_loss) / (num - 1)\n",
    "        if gt < best_alpha:\n",
    "            best_alpha, best_p, best_t = gt, p, tree\n",
    "    \n",
    "    if tree_type == \"regressor\":\n",
    "        if tree.left.type == \"internal\":\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.left, tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        if tree.right.type == \"internal\":\n",
    "            sub_alpha, sub_p, sub_t = get_optimal_tree(tree.right, tree, alpha, tree_type)\n",
    "            if best_alpha > sub_alpha:\n",
    "                best_alpha, best_p, best_t = sub_alpha, sub_p, sub_t\n",
    "        node_loss = compute_node_loss(tree, tree_type)\n",
    "        tree_loss, num = compute_tree_loss_and_leaf(tree, tree_type)\n",
    "        gt = (node_loss - tree_loss) / (num - 1)\n",
    "        if gt < best_alpha:\n",
    "            best_alpha, best_p, best_t = gt, p, tree\n",
    "    \n",
    "    return (best_alpha, best_p, best_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_cart(tree0, val_X, val_y, tree_type):\n",
    "    \"\"\"\n",
    "    对CART树进行剪枝\n",
    "    参数：\n",
    "        tree0: CART树T_0\n",
    "        val_X: 交叉验证集X\n",
    "        val_y: 交叉验证集y\n",
    "        tree_type: 树的类型，\"classifier\": 决策树，\"regressor\": 回归树\n",
    "    返回：\n",
    "        最优子树T\n",
    "    \"\"\"\n",
    "    if tree_type == \"classifier\":\n",
    "        if type(tree0) == LeafNode:\n",
    "            return tree0\n",
    "        if type(tree0.branch[0]) == LeafNode and type(tree0.branch[1]) == LeafNode:\n",
    "            return tree0\n",
    "    if tree_type == \"regressor\":\n",
    "        if tree0.type == \"leaf\":\n",
    "            return tree0\n",
    "        if tree0.left.type == \"leaf\" and tree0.right.type == \"leaf\":\n",
    "            return tree0\n",
    "    \n",
    "    T_list = [tree0]\n",
    "    alpha_list = [0]\n",
    "    while True:\n",
    "        T = copy.deepcopy(T_list[-1])\n",
    "        root = T\n",
    "        alpha, p, best_t = get_optimal_tree(T, None, sys.maxsize, tree_type)\n",
    "        # 裁剪t\n",
    "        t = None\n",
    "        if tree_type == \"classifier\":\n",
    "            t = LeafNode()\n",
    "            t.train_classes_num = best_t.train_classes_num\n",
    "            t.val_classes_num = best_t.val_classes_num\n",
    "            t.data_info = best_t.data_info\n",
    "            t.classs_label = get_max_num_class(t.train_classes_num)\n",
    "        if tree_type == \"regressor\":\n",
    "            t = RNode()\n",
    "            t.type = \"leaf\"\n",
    "            t.y = best_t.y\n",
    "            t.predict = best_t.predict\n",
    "            \n",
    "        alpha_list.append(alpha)\n",
    "        # 根结点也可以是叶子结点\n",
    "        if best_t == root:\n",
    "            T_list.append(t)\n",
    "            break\n",
    "        else:\n",
    "            if tree_type == \"classifier\":\n",
    "                if p.branch[0] == best_t:\n",
    "                    p.branch[0] = t\n",
    "                if p.branch[1] == best_t:\n",
    "                    p.branch[1] = t\n",
    "            if tree_type == \"regressor\":\n",
    "                if p.left == best_t:\n",
    "                    p.left = t\n",
    "                if p.right == best_t:\n",
    "                    p.right = t\n",
    "            T_list.append(T)\n",
    "\n",
    "    # 采用交叉验证法在子树序列T_list中选取最优子树T\n",
    "    best_T = None\n",
    "    if tree_type == \"classifier\":        \n",
    "        best_acc = 0\n",
    "        for T in T_list:\n",
    "            y_predict = []\n",
    "            for i in range(len(val_X)):\n",
    "                predict = cart_classifier_inference(T, val_X[i])\n",
    "                y_predict.append(1 if predict == val_y[i] else 0)\n",
    "            acc = np.mean(y_predict)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_T = T\n",
    "    if tree_type == \"regressor\":\n",
    "        best_loss = sys.maxsize\n",
    "        for T in T_list:\n",
    "            y_predict = []\n",
    "            for i in range(len(val_X)):\n",
    "                y_predict.append(cart_regressor_inference(T, val_X[i]))\n",
    "            y_predict = np.array(y_predict).reshape(-1, 1)\n",
    "            loss = np.mean(np.power(val_y - y_predict, 2))\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_T = T\n",
    "       \n",
    "    return best_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用CART回归树模拟sin函数\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.linspace(-5, 5, 200)\n",
    "y = np.sin(X)\n",
    "y = y + np.random.rand(1, len(y)) * 1.5 # 加入噪声\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "np.random.seed(0)\n",
    "indicate = np.random.rand(200) <= 0.7\n",
    "\n",
    "X_train = X[indicate].reshape(-1, 1)\n",
    "y_train = y[indicate].reshape(-1, 1)\n",
    "\n",
    "X_val = X[~indicate].reshape(-1, 1)\n",
    "y_val = y[~indicate].reshape(-1, 1)\n",
    "\n",
    "regressor_before = generate_cart_regressor(X_train, y_train, 1, 5)\n",
    "\n",
    "X_show = np.arange(-5, 5, 0.05).reshape(-1, 1)\n",
    "y_predict_before = np.zeros((X_show.shape[0], 1))\n",
    "\n",
    "for i in range(X_show.shape[0]):\n",
    "    y_predict_before[i, 0] = cart_regressor_inference(regressor_before, X_show[i])\n",
    "\n",
    "regressor_after = pruning_cart(regressor_before, X_val, y_val, \"regressor\")\n",
    "\n",
    "y_predict_after = np.zeros((X_show.shape[0], 1))\n",
    "\n",
    "for i in range(X_show.shape[0]):\n",
    "    y_predict_after[i, 0] = cart_regressor_inference(regressor_before, X_show[i])\n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "plt.scatter(X, y,c=\"k\", label=\"data\")\n",
    "plt.plot(X_show, y_predict_before, c=\"r\", label=\"before\", linewidth=2)\n",
    "plt.plot(X_show, y_predict_before, c=\"b\", label=\"after\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类树剪枝\n",
    "m = len(data_set)\n",
    "data_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "for i in range(int(m * 0.7)):\n",
    "    data_train.append(data_set[i])\n",
    "    \n",
    "for i in range(int(m * 0.7), m):\n",
    "    X_val.append(data_set[i][0:-1])\n",
    "    y_val.append(data_set[i][-1])\n",
    "\n",
    "classifier_before = generate_cart_classifier(data_train, data_info)\n",
    "#print(json.dumps(eval(str(classifier_before)), sort_keys=True, indent=2))\n",
    "classifier_after = pruning_cart(classifier_before, X_val, y_val, \"classifier\")\n",
    "print(json.dumps(eval(str(classifier_before)), sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
