{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression \n",
    "在这个实验中，我们将通过对比未进行正则化的Logistic回归与正则化的Logistic回归在相同数据集上的表现来理解正则化缓解过拟合现象的作用。\n",
    "## 1. 导入Python库\n",
    "首先，我们导入这次实验所需要使用的Python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import savefig\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 可视化数据\n",
    "接下来，我们导入这次实验需要用到的数据，并且对其进行可视化。  \n",
    "设$X$为我们的特征矩阵，$x^{(i)}$为训练集里面的第$i$个样本，$x_j$为样本中的第$j$个特征，则：  \n",
    "$$X=\\begin{bmatrix}x_1^{(1)} & x_2^{(1)} \\\\ x_1^{(2)} & x_2^{(2)} \\\\ \\vdots & \\vdots \\\\ x_1^{(m)} & x_2^{(m)} \\end{bmatrix}$$  \n",
    "$Y$为一个列向量，$y^{(i)}$代表第$i$个样本对应的标签，则：  \n",
    "$$Y=\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('circle_data.txt', delimiter=' ')\n",
    "X = data[:, 0:2].reshape(-1,2)\n",
    "Y = data[:, 2].reshape(-1,1)\n",
    "\n",
    "print(\"The shape of X is:\", X.shape)\n",
    "print(\"The shape of Y is:\", Y.shape)\n",
    "\n",
    "def plotData(X, y):\n",
    "    # Find Indices of Positive and Negative Examples\n",
    "    pos = np.where(y == 1)\n",
    "    neg = np.where(y == 0)\n",
    "    plt.scatter(X[pos,0], X[pos,1], c='b')\n",
    "    plt.scatter(X[neg,0], X[neg,1], c='r')\n",
    "    plt.savefig('./oringnal_data.png')\n",
    "    return plt\n",
    "plotData(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sigmoid函数\n",
    "我们打算使用Logistic回归训练一个模型，来区分我们的正类与负类，因此我们需要一个Sigmoid函数：  \n",
    "$$sigmoid(z) = \\frac{1}{1+e^{-z}}$$\n",
    "**注意**：我们写的Sigmoid函数是需要能够对矩阵直接进行操作的。  \n",
    "**Hint**：计算$e^{-z}$可以使用np.exp(-z)来进行计算  \n",
    "**任务1**：实现sigmoid函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    对矩阵z中每个元素计算其Sigmoid函数值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    g = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return g\n",
    "\n",
    "#测试一下我们的sigmoid函数是否正确，运行下列代码\n",
    "print(sigmoid(X[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature mapping\n",
    "这次实验只选取了两个特征$x_1$和$x_2$，为了能够得到更加复杂的拟合曲线，我们需要通过这两个特征映射出更多的特征，具体方法是将这两个特征映射所有$x_1$和$x_2$的不超过6次方的多项式中，即：  \n",
    "$$mapFeature(x_1,x_2)=\\begin{bmatrix}1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_1x_2 \\\\ x_2^2 \\\\ \\vdots \\\\ x_1x_2^5 \\\\ x_2^6 \\end{bmatrix}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1, X2):\n",
    "    \"\"\"\n",
    "    特征映射函数，从X1和X2中映射出更多特征\n",
    "    \"\"\"\n",
    "    degree = 6    #最高为6次幂\n",
    "    newX = np.ones((X1.shape[0], sum(range(degree + 2))))    #初始化新的特征矩阵\n",
    "    end = 1\n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(0, i+1):\n",
    "            newX[:, end] = np.multiply(np.power(X1, i-j), np.power(X2, j))\n",
    "            end = end + 1\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码对输入$X$中的两个特征进行特征映射，我们可以检查newX的维度来确认是否正确。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX = mapFeature(X[:, 0], X[:, 1])\n",
    "print(\"After mapping the features, the shape of newX is:\", newX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 计算成本与梯度\n",
    "首先，我们初始化权重$\\theta$为零向量。  \n",
    "$$\\theta = \\begin{bmatrix}\\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} $$  \n",
    "其中$n$为特征的数量。  \n",
    "**Hint**：使用np.zeros()  \n",
    "**任务2**：初始化权重$\\theta$为零向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化theta为零向量\n",
    "### START CODE HERE ###\n",
    "\n",
    "initial_theta = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "print(\"The initialized theta's shape is:\", initial_theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要计算成本与梯度，回忆一下我们上课学过的内容，加入了正则化的成本函数为：  \n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}{[y^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]}+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}{\\theta_{j}^2}$$  \n",
    "其中，$$h_\\theta(X)=g(X\\theta)\\\\ g(z) = sigmoid(z)$$\n",
    "$\\frac{\\lambda}{2m}\\sum_{j=1}^{n}{\\theta_{j}^2}$是正则化项。注意到在上述公式中，我们不需要对$\\theta_0$的权重进行惩罚，所以索引$j$从1开始。  \n",
    "然后梯度的计算公式为：  \n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0}= \\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}\\qquad j=0$$  \n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=  \\big(\\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}\\big)+\\frac{\\lambda}{m}\\theta_{j} \\quad j\\in\\left\\{ 1,2,...n \\right\\}$$  \n",
    "**Hint**：这些函数或许可以帮助你：np.dot(),np.log(),np.power(),sum().  \n",
    "**任务3**：完成成本函数后，计算一下初始为零向量的$\\theta$所对应的成本与梯度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(X, y, theta, lambd):\n",
    "    \"\"\"\n",
    "    计算成本J与梯度grad\n",
    "    :param theta:权重矩阵theta\n",
    "    :param X:特征矩阵X\n",
    "    :param y:特征矩阵X对应的标签\n",
    "    :param lambd:正则化参数lambda\n",
    "    :return:成本与梯度\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    m = None\n",
    "\n",
    "    h = None\n",
    "    J = None\n",
    "\n",
    "    grad = None\n",
    "    grad[0] = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "#我们来计算一下初始为零向量的theta所对应的成本与梯度值（先不加入正则化项，即lambda = 0）  \n",
    "initial_J, initial_grad = costFunctionReg(newX, Y, initial_theta, lambd = 0)\n",
    "print(\"The cost is:\", initial_J)\n",
    "print(\"The 5th element of the gradient is:\", initial_grad[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 更新参数$\\theta$\n",
    "参数$\\theta$更新的公式为：  \n",
    "$$\\theta_{0} := \\theta_{0} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}$$  \n",
    "$$\\theta_{j} := \\theta_{j} - \\alpha \\left[ (\\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}})+\\frac{\\lambda}{m}\\theta_{j} \\right] \\quad j\\in\\left\\{ 1,2,...n \\right\\}$$    \n",
    "**任务4**：完成参数更新函数后，计算第一次更新参数后的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_Parameter(theta, gradients, learning_rate = 0.5):\n",
    "    \"\"\"\n",
    "    更新参数theta\n",
    "    :param theta:权重theta\n",
    "    :param gradients:梯度值\n",
    "    :param learning_rate:学习速率\n",
    "    :return:更新后的theta\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    theta = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return theta\n",
    "\n",
    "#我们试着利用刚才计算得到的梯度值更新我们的参数theta\n",
    "print(\"After update theta, now, the 5th element of theta will be:\", Update_Parameter(initial_theta,  initial_grad)[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定义模型\n",
    "接下来，我们将上面的代码整合到我们的模型Model中，并且我们将记录下成本$J$的变化过程。  \n",
    "**任务5**：完成模型函数后，训练一个未进行正则化的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(X, y, theta, iteration=500000, learning_rate = 0.5, lambd = 1):\n",
    "    \"\"\"\n",
    "    Regulared Logistic Regression Model\n",
    "    :param X:输入X\n",
    "    :param y:标签Y\n",
    "    :param theta:参数theta\n",
    "    :param iteration:迭代次数\n",
    "    :param learning_rate:学习率\n",
    "    :return:成本J的历史记录J_history和最终theta的值\n",
    "    \"\"\"\n",
    "    J_history = []\n",
    "\n",
    "    for i in range(iteration):\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        J , grad = None\n",
    "        theta = None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        J_history.append(J)\n",
    "        \n",
    "    return J_history, theta\n",
    "\n",
    "#我们先训练一个未进行正则化的模型，训练过程可能会花费一定的时间。  \n",
    "#初始化theta\n",
    "### START CODE HERE ###\n",
    "\n",
    "theta = None\n",
    "#对模型进行训练\n",
    "J_history, theta = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "#输出最终的theta值\n",
    "print(\"At the end, the value of theta is:\", theta)\n",
    "#绘制出成本值的历史曲线\n",
    "plt.plot(J_history)\n",
    "plt.savefig('./J_history1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 绘制决策边界\n",
    "下面的代码直接运行即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(theta, X, y, image):\n",
    "    plt = plotData(newX[:, 1:3], y)\n",
    "    \n",
    "    u = np.linspace(-1.5, 1.5, 100)\n",
    "    v = np.linspace(-1.5, 1.5, 100)\n",
    "\n",
    "    z = np.zeros((len(u), len(v)))\n",
    "    # Evaluate z = theta*x over the grid\n",
    "    for i in range(len(u)):\n",
    "        for j in range(len(v)):\n",
    "            z[i, j] = np.dot(mapFeature(np.array([u[i]]), np.array([v[j]])), theta)\n",
    "    z = np.transpose(z) \n",
    "    p3 = plt.contour(u, v, z, levels=[0]).collections[0]\n",
    "    filename = 'boundary' + str(image) + '.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们绘制出刚才训练好的未进行正则化的模型的决策边界。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDecisionBoundary(theta, X, Y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 训练正则化的模型\n",
    "接下来我们训练一个正则化的Logistic Regression模型，我们将$\\lambda$设置为1  \n",
    "**任务6**：训练正则化的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化theta\n",
    "### START CODE HERE ###\n",
    "\n",
    "theta = None\n",
    "#对模型进行训练\n",
    "J_history, theta = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "#输出最终的theta值\n",
    "print(\"At the end, the value of theta is:\", theta)\n",
    "#绘制出成本值的历史曲线\n",
    "plt.plot(J_history)\n",
    "plt.savefig('./J_history2.png')\n",
    "plt.show()\n",
    "#绘制出决策边界\n",
    "plotDecisionBoundary(theta, X, Y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "我们对比一下上面的两个决策边界，未进行正则化的模型曲线看起来十分复杂，似乎是有意的框出这两种不同颜色的点，虽然过拟合的模型能够在训练集上获得更高的准确率，但是这种模型的泛化能力往往很不理想。反过来，我们看进行了正则化的模型，它的曲线似乎圆润了许多，因此泛化能力要更好些。  \n",
    "通过这次实验，我们能够清晰的理解正则化对于缓解过拟合现象所起到的作用。在提交完作业后，你还可以试试不同的$\\lambda$值，观察决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
